{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb176769",
   "metadata": {},
   "source": [
    "# DPGAN vs K-Anonymity on Adult Dataset\n",
    "This notebook compares the performance of DNN classifiers trained on:\n",
    "- Original data\n",
    "- K-Anonymized data (using Mondrian algorithm)\n",
    "- Synthetic data generated by a DP-GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2388b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df.copy().apply(LabelEncoder().fit_transform)\n",
    "    X = df.drop(columns=[\"income\"])\n",
    "    y = df[\"income\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X_train), scaler.transform(X_test), y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(X_train, X_test, y_train, y_test):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    y_prob = model.predict(X_test).flatten()\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_prob),\n",
    "        \"Misclassification Error\": 1 - accuracy_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MondrianKAnonymity:\n",
    "    def __init__(self, k): self.k = k\n",
    "\n",
    "    def partition(self, data, quasi_identifiers):\n",
    "        if len(data) < 2 * self.k: return [data]\n",
    "        attr = self._max_spread_attribute(data, quasi_identifiers)\n",
    "        median = np.median(data[attr])\n",
    "        left = data[data[attr] <= median]\n",
    "        right = data[data[attr] > median]\n",
    "        if len(left) < self.k or len(right) < self.k: return [data]\n",
    "        return self.partition(left, quasi_identifiers) + self.partition(right, quasi_identifiers)\n",
    "\n",
    "    def _max_spread_attribute(self, data, quasi_identifiers):\n",
    "        return max(quasi_identifiers, key=lambda attr: data[attr].max() - data[attr].min())\n",
    "\n",
    "    def anonymize(self, data, quasi_identifiers):\n",
    "        partitions = self.partition(data, quasi_identifiers)\n",
    "        result = []\n",
    "        for p in partitions:\n",
    "            gen_vals = {q: f\"[{p[q].min()}-{p[q].max()}]\" for q in quasi_identifiers}\n",
    "            for _, row in p.iterrows():\n",
    "                row_copy = row.copy()\n",
    "                for q in quasi_identifiers:\n",
    "                    row_copy[q] = gen_vals[q]\n",
    "                result.append(row_copy)\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "def convert_to_numeric(df, quasi_identifiers):\n",
    "    for col in quasi_identifiers:\n",
    "        df[col] = df[col].apply(lambda x: np.mean([float(num) for num in x.strip(\"[]\").split(\"-\")]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPGAN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim=100):\n",
    "        super(DPGAN, self).__init__()\n",
    "        self.generator = self.make_generator(latent_dim, input_dim)\n",
    "        self.discriminator = self.make_discriminator(input_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def make_generator(self, latent_dim, output_dim):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(latent_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(output_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def make_discriminator(self, input_dim):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def train(self, real_data, epochs=500, batch_size=64, epsilon=1.0, delta=1e-5):\n",
    "        optimizer_d = tf.keras.optimizers.Adam(1e-4)\n",
    "        optimizer_g = tf.keras.optimizers.Adam(1e-4)\n",
    "        data = tf.data.Dataset.from_tensor_slices(real_data).shuffle(10000).batch(batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for real_batch in data:\n",
    "                noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "                with tf.GradientTape() as tape_d:\n",
    "                    fake = self.generator(noise, training=True)\n",
    "                    logits_real = self.discriminator(real_batch, training=True)\n",
    "                    logits_fake = self.discriminator(fake, training=True)\n",
    "                    loss_d = tf.reduce_mean(logits_fake) - tf.reduce_mean(logits_real)\n",
    "                grads_d = tape_d.gradient(loss_d, self.discriminator.trainable_variables)\n",
    "                optimizer_d.apply_gradients(zip(grads_d, self.discriminator.trainable_variables))\n",
    "\n",
    "                noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "                with tf.GradientTape() as tape_g:\n",
    "                    fake = self.generator(noise, training=True)\n",
    "                    logits_fake = self.discriminator(fake, training=True)\n",
    "                    loss_g = -tf.reduce_mean(logits_fake)\n",
    "                grads_g = tape_g.gradient(loss_g, self.generator.trainable_variables)\n",
    "                optimizer_g.apply_gradients(zip(grads_g, self.generator.trainable_variables))\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        noise = tf.random.normal((num_samples, self.latent_dim))\n",
    "        return self.generator(noise).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\drlee\\\\Downloads\\\\adult\\\\adult.data\", names=columns, sep=\", \", engine=\"python\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Original\n",
    "X_train, X_test, y_train, y_test = preprocess_data(data)\n",
    "results[\"Original DNN\"] = train_dnn(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# K-Anonymity\n",
    "k = 5\n",
    "quasi_identifiers = [\"age\", \"education-num\", \"hours-per-week\"]\n",
    "k_anon = MondrianKAnonymity(k)\n",
    "k_anon_data = k_anon.anonymize(data, quasi_identifiers)\n",
    "k_anon_data = convert_to_numeric(k_anon_data, quasi_identifiers)\n",
    "X_train_k, X_test_k, y_train_k, y_test_k = preprocess_data(k_anon_data)\n",
    "results[\"K-Anonymity DNN\"] = train_dnn(X_train_k, X_test_k, y_train_k, y_test_k)\n",
    "\n",
    "# DP-GAN\n",
    "df_encoded = data.copy().apply(LabelEncoder().fit_transform)\n",
    "X_data = df_encoded.drop(columns=[\"income\"])\n",
    "y_data = df_encoded[\"income\"]\n",
    "dpgan = DPGAN(input_dim=X_data.shape[1])\n",
    "dpgan.train(X_data.values, epochs=200)\n",
    "synthetic_data = dpgan.sample(10000)\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=X_data.columns)\n",
    "synthetic_df[\"income\"] = np.random.choice([0, 1], size=(len(synthetic_df),))\n",
    "synthetic_df = synthetic_df[columns]\n",
    "X_train_dp, X_test_dp, y_train_dp, y_test_dp = preprocess_data(synthetic_df)\n",
    "results[\"DP-GAN DNN\"] = train_dnn(X_train_dp, X_test_dp, y_train_dp, y_test_dp)\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
