{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11448087,"sourceType":"datasetVersion","datasetId":7172366}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bb176769","cell_type":"markdown","source":"# DPGAN vs K-Anonymity on Adult Dataset\nThis notebook compares the performance of DNN classifiers trained on:\n- Original data\n- K-Anonymized data (using Mondrian algorithm)\n- Synthetic data generated by a DP-GAN.","metadata":{}},{"id":"b1cb69c0","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.891Z"}},"outputs":[],"execution_count":null},{"id":"b2388b4b","cell_type":"code","source":"def preprocess_data(df):\n    df = df.copy().apply(LabelEncoder().fit_transform)\n    X = df.drop(columns=[\"income\"])\n    y = df[\"income\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    return scaler.fit_transform(X_train), scaler.transform(X_test), y_train, y_test","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.891Z"}},"outputs":[],"execution_count":null},{"id":"9ef6edd8","cell_type":"code","source":"def train_dnn(X_train, X_test, y_train, y_test):\n    model = models.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n                  loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n    y_prob = model.predict(X_test).flatten()\n    return {\n        \"Accuracy\": accuracy_score(y_test, y_pred),\n        \"Precision\": precision_score(y_test, y_pred),\n        \"Recall\": recall_score(y_test, y_pred),\n        \"F1-score\": f1_score(y_test, y_pred),\n        \"AUC\": roc_auc_score(y_test, y_prob),\n        \"Misclassification Error\": 1 - accuracy_score(y_test, y_pred)\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.891Z"}},"outputs":[],"execution_count":null},{"id":"a08a295b","cell_type":"code","source":"class MondrianKAnonymity:\n    def __init__(self, k): self.k = k\n\n    def partition(self, data, quasi_identifiers):\n        if len(data) < 2 * self.k: return [data]\n        attr = self._max_spread_attribute(data, quasi_identifiers)\n        median = np.median(data[attr])\n        left = data[data[attr] <= median]\n        right = data[data[attr] > median]\n        if len(left) < self.k or len(right) < self.k: return [data]\n        return self.partition(left, quasi_identifiers) + self.partition(right, quasi_identifiers)\n\n    def _max_spread_attribute(self, data, quasi_identifiers):\n        return max(quasi_identifiers, key=lambda attr: data[attr].max() - data[attr].min())\n\n    def anonymize(self, data, quasi_identifiers):\n        partitions = self.partition(data, quasi_identifiers)\n        result = []\n        for p in partitions:\n            gen_vals = {q: f\"[{p[q].min()}-{p[q].max()}]\" for q in quasi_identifiers}\n            for _, row in p.iterrows():\n                row_copy = row.copy()\n                for q in quasi_identifiers:\n                    row_copy[q] = gen_vals[q]\n                result.append(row_copy)\n        return pd.DataFrame(result)\n\ndef convert_to_numeric(df, quasi_identifiers):\n    for col in quasi_identifiers:\n        df[col] = df[col].apply(lambda x: np.mean([float(num) for num in x.strip(\"[]\").split(\"-\")]))\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.891Z"}},"outputs":[],"execution_count":null},{"id":"ade2752d","cell_type":"code","source":"class DPGAN(tf.keras.Model):\n    def __init__(self, input_dim, latent_dim=100):\n        super(DPGAN, self).__init__()\n        self.generator = self.make_generator(latent_dim, input_dim)\n        self.discriminator = self.make_discriminator(input_dim)\n        self.latent_dim = latent_dim\n        self.input_dim = input_dim\n\n    def make_generator(self, latent_dim, output_dim):\n        return tf.keras.Sequential([\n            layers.Dense(128, activation='relu', input_shape=(latent_dim,)),\n            layers.Dense(256, activation='relu'),\n            layers.Dense(output_dim, activation='sigmoid')\n        ])\n\n    def make_discriminator(self, input_dim):\n        return tf.keras.Sequential([\n            layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n            layers.Dense(128, activation='relu'),\n            layers.Dense(1)\n        ])\n\n    def train(self, real_data, epochs=500, batch_size=64, epsilon=1.0, delta=1e-5):\n        optimizer_d = tf.keras.optimizers.Adam(1e-4)\n        optimizer_g = tf.keras.optimizers.Adam(1e-4)\n        data = tf.data.Dataset.from_tensor_slices(real_data).shuffle(10000).batch(batch_size)\n\n        for epoch in range(epochs):\n            for real_batch in data:\n                noise = tf.random.normal((batch_size, self.latent_dim))\n                with tf.GradientTape() as tape_d:\n                    fake = self.generator(noise, training=True)\n                    logits_real = self.discriminator(real_batch, training=True)\n                    logits_fake = self.discriminator(fake, training=True)\n                    loss_d = tf.reduce_mean(logits_fake) - tf.reduce_mean(logits_real)\n                grads_d = tape_d.gradient(loss_d, self.discriminator.trainable_variables)\n                optimizer_d.apply_gradients(zip(grads_d, self.discriminator.trainable_variables))\n\n                noise = tf.random.normal((batch_size, self.latent_dim))\n                with tf.GradientTape() as tape_g:\n                    fake = self.generator(noise, training=True)\n                    logits_fake = self.discriminator(fake, training=True)\n                    loss_g = -tf.reduce_mean(logits_fake)\n                grads_g = tape_g.gradient(loss_g, self.generator.trainable_variables)\n                optimizer_g.apply_gradients(zip(grads_g, self.generator.trainable_variables))\n\n    def sample(self, num_samples):\n        noise = tf.random.normal((num_samples, self.latent_dim))\n        return self.generator(noise).numpy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.892Z"}},"outputs":[],"execution_count":null},{"id":"c46c856a","cell_type":"code","source":"columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n           \"hours-per-week\", \"native-country\", \"income\"]\ndata = pd.read_csv(\"/kaggle/input/adultdata/adult.data\", names=columns, sep=\", \", engine=\"python\")\n\nresults = {}\n\n# Original\nX_train, X_test, y_train, y_test = preprocess_data(data)\nresults[\"Original DNN\"] = train_dnn(X_train, X_test, y_train, y_test)\n\n# K-Anonymity\nk = 5\nquasi_identifiers = [\"age\", \"education-num\", \"hours-per-week\"]\nk_anon = MondrianKAnonymity(k)\nk_anon_data = k_anon.anonymize(data, quasi_identifiers)\nk_anon_data = convert_to_numeric(k_anon_data, quasi_identifiers)\nX_train_k, X_test_k, y_train_k, y_test_k = preprocess_data(k_anon_data)\nresults[\"K-Anonymity DNN\"] = train_dnn(X_train_k, X_test_k, y_train_k, y_test_k)\n\n# DP-GAN\ndf_encoded = data.copy().apply(LabelEncoder().fit_transform)\nX_data = df_encoded.drop(columns=[\"income\"])\ny_data = df_encoded[\"income\"]\ndpgan = DPGAN(input_dim=X_data.shape[1])\ndpgan.train(X_data.values, epochs=200)\nsynthetic_data = dpgan.sample(10000)\nsynthetic_df = pd.DataFrame(synthetic_data, columns=X_data.columns)\nsynthetic_df[\"income\"] = np.random.choice([0, 1], size=(len(synthetic_df),))\nsynthetic_df = synthetic_df[columns]\nX_train_dp, X_test_dp, y_train_dp, y_test_dp = preprocess_data(synthetic_df)\nresults[\"DP-GAN DNN\"] = train_dnn(X_train_dp, X_test_dp, y_train_dp, y_test_dp)\n\nfor model, metrics in results.items():\n    print(f\"\\n{model}:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-17T12:48:34.892Z"}},"outputs":[],"execution_count":null}]}