{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11448087,"sourceType":"datasetVersion","datasetId":7172366}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bb176769","cell_type":"markdown","source":"# DPGAN vs K-Anonymity on Adult Dataset\nThis notebook compares the performance of DNN classifiers trained on:\n- Original data\n- K-Anonymized data (using Mondrian algorithm)\n- Synthetic data generated by a DP-GAN.","metadata":{}},{"id":"b1cb69c0","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:07.426129Z","iopub.execute_input":"2025-04-17T15:06:07.426820Z","iopub.status.idle":"2025-04-17T15:06:11.089616Z","shell.execute_reply.started":"2025-04-17T15:06:07.426784Z","shell.execute_reply":"2025-04-17T15:06:11.089056Z"}},"outputs":[{"name":"stderr","text":"2025-04-17 15:06:08.000897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744902368.023161    3289 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744902368.029942    3289 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"b2388b4b","cell_type":"code","source":"# ============ Data Preprocessing ============\ndef preprocess_data(df):\n    df = df.copy().apply(LabelEncoder().fit_transform)\n    X = df.drop(columns=[\"income\"])\n    y = df[\"income\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    return scaler.fit_transform(X_train), scaler.transform(X_test), y_train, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:11.090794Z","iopub.execute_input":"2025-04-17T15:06:11.091321Z","iopub.status.idle":"2025-04-17T15:06:11.096052Z","shell.execute_reply.started":"2025-04-17T15:06:11.091291Z","shell.execute_reply":"2025-04-17T15:06:11.095344Z"}},"outputs":[],"execution_count":2},{"id":"9ef6edd8","cell_type":"code","source":"# ============ DNN Model ============\ndef train_dnn(X_train, X_test, y_train, y_test):\n    model = models.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.002),\n                  loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n    y_prob = model.predict(X_test).flatten()\n    return {\n        \"Accuracy\": accuracy_score(y_test, y_pred),\n        \"Precision\": precision_score(y_test, y_pred),\n        \"Recall\": recall_score(y_test, y_pred),\n        \"F1-score\": f1_score(y_test, y_pred),\n        \"AUC\": roc_auc_score(y_test, y_prob),\n        \"Misclassification Error\": 1 - accuracy_score(y_test, y_pred)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:11.096686Z","iopub.execute_input":"2025-04-17T15:06:11.096885Z","iopub.status.idle":"2025-04-17T15:06:11.110912Z","shell.execute_reply.started":"2025-04-17T15:06:11.096869Z","shell.execute_reply":"2025-04-17T15:06:11.110363Z"}},"outputs":[],"execution_count":3},{"id":"a08a295b","cell_type":"code","source":"# ============ Mondrian K-Anonymity ============\nclass MondrianKAnonymity:\n    def __init__(self, k): self.k = k\n    def partition(self, data, quasi_identifiers):\n        if len(data) < 2 * self.k: return [data]\n        attr = self._max_spread_attribute(data, quasi_identifiers)\n        median = np.median(data[attr])\n        left = data[data[attr] <= median]\n        right = data[data[attr] > median]\n        if len(left) < self.k or len(right) < self.k: return [data]\n        return self.partition(left, quasi_identifiers) + self.partition(right, quasi_identifiers)\n    def _max_spread_attribute(self, data, quasi_identifiers):\n        return max(quasi_identifiers, key=lambda attr: data[attr].max() - data[attr].min())\n    def anonymize(self, data, quasi_identifiers):\n        partitions = self.partition(data, quasi_identifiers)\n        result = []\n        for p in partitions:\n            gen_vals = {q: f\"[{p[q].min()}-{p[q].max()}]\" for q in quasi_identifiers}\n            for _, row in p.iterrows():\n                row_copy = row.copy()\n                for q in quasi_identifiers:\n                    row_copy[q] = gen_vals[q]\n                result.append(row_copy)\n        return pd.DataFrame(result)\n\ndef convert_to_numeric(df, quasi_identifiers):\n    for col in quasi_identifiers:\n        df[col] = df[col].apply(lambda x: np.mean([float(i) for i in x.strip(\"[]\").split(\"-\")]))\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:11.111626Z","iopub.execute_input":"2025-04-17T15:06:11.111827Z","iopub.status.idle":"2025-04-17T15:06:11.128475Z","shell.execute_reply.started":"2025-04-17T15:06:11.111811Z","shell.execute_reply":"2025-04-17T15:06:11.127852Z"}},"outputs":[],"execution_count":4},{"id":"ade2752d","cell_type":"code","source":"# ============ Differentially Private GAN ============\nclass DPGAN(tf.keras.Model):\n    def __init__(self, input_dim, latent_dim=100, clip_norm=1.0, noise_multiplier=1.1):\n        super(DPGAN, self).__init__()\n        self.latent_dim = latent_dim\n        self.input_dim = input_dim\n        self.clip_norm = clip_norm\n        self.noise_multiplier = noise_multiplier\n        self.generator = self.build_generator()\n        self.discriminator = self.build_discriminator()\n\n    def build_generator(self):\n        return tf.keras.Sequential([\n            layers.Dense(128, activation='relu', input_shape=(self.latent_dim,)),\n            layers.Dense(256, activation='relu'),\n            layers.Dense(self.input_dim, activation='sigmoid')\n        ])\n\n    def build_discriminator(self):\n        return tf.keras.Sequential([\n            layers.Dense(256, activation='relu', input_shape=(self.input_dim,)),\n            layers.Dense(128, activation='relu'),\n            layers.Dense(1)\n        ])\n\n    def _apply_dp(self, grads):\n        clipped_grads = [tf.clip_by_norm(g, self.clip_norm) for g in grads]\n        noised_grads = [g + tf.random.normal(tf.shape(g), stddev=self.noise_multiplier * self.clip_norm)\n                        for g in clipped_grads]\n        return noised_grads\n\n    def train(self, real_data, epochs=200, batch_size=64):\n        dataset = tf.data.Dataset.from_tensor_slices(real_data).shuffle(10000).batch(batch_size)\n        optimizer_g = tf.keras.optimizers.Adam(1e-4)\n        optimizer_d = tf.keras.optimizers.Adam(1e-4)\n\n        for epoch in range(epochs):\n            for real_batch in dataset:\n                noise = tf.random.normal((batch_size, self.latent_dim))\n                with tf.GradientTape() as tape_d:\n                    fake = self.generator(noise)\n                    logits_real = self.discriminator(real_batch)\n                    logits_fake = self.discriminator(fake)\n                    loss_d = tf.reduce_mean(logits_fake) - tf.reduce_mean(logits_real)\n                grads_d = tape_d.gradient(loss_d, self.discriminator.trainable_variables)\n                dp_grads_d = self._apply_dp(grads_d)\n                optimizer_d.apply_gradients(zip(dp_grads_d, self.discriminator.trainable_variables))\n\n                noise = tf.random.normal((batch_size, self.latent_dim))\n                with tf.GradientTape() as tape_g:\n                    fake = self.generator(noise)\n                    logits_fake = self.discriminator(fake)\n                    loss_g = -tf.reduce_mean(logits_fake)\n                grads_g = tape_g.gradient(loss_g, self.generator.trainable_variables)\n                dp_grads_g = self._apply_dp(grads_g)\n                optimizer_g.apply_gradients(zip(dp_grads_g, self.generator.trainable_variables))\n\n    def sample(self, num_samples):\n        noise = tf.random.normal((num_samples, self.latent_dim))\n        return self.generator(noise).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:11.129929Z","iopub.execute_input":"2025-04-17T15:06:11.130150Z","iopub.status.idle":"2025-04-17T15:06:11.146913Z","shell.execute_reply.started":"2025-04-17T15:06:11.130126Z","shell.execute_reply":"2025-04-17T15:06:11.146159Z"}},"outputs":[],"execution_count":5},{"id":"c46c856a","cell_type":"code","source":"# ============ Main Execution ============\nif __name__ == \"__main__\":\n    # Load dataset\n    columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n               \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n               \"hours-per-week\", \"native-country\", \"income\"]\n    data = pd.read_csv(\"/kaggle/input/adultdata/adult.data\", names=columns, sep=\", \", engine=\"python\")\n\n    results = {}\n\n    # 1. Original\n    X_train, X_test, y_train, y_test = preprocess_data(data)\n    results[\"Original DNN\"] = train_dnn(X_train, X_test, y_train, y_test)\n\n    # 2. Mondrian K-Anonymity\n    k = 5\n    qid = [\"age\", \"education-num\", \"hours-per-week\"]\n    k_anon = MondrianKAnonymity(k)\n    anon_data = k_anon.anonymize(data, qid)\n    anon_data = convert_to_numeric(anon_data, qid)\n    X_train_k, X_test_k, y_train_k, y_test_k = preprocess_data(anon_data)\n    results[\"K-Anonymity DNN\"] = train_dnn(X_train_k, X_test_k, y_train_k, y_test_k)\n\n    # 3. DP-GAN\n    df_enc = data.copy().apply(LabelEncoder().fit_transform)\n    X_data = df_enc.drop(columns=[\"income\"])\n    y_data = df_enc[\"income\"]\n    dpgan = DPGAN(input_dim=X_data.shape[1])\n    dpgan.train(X_data.values)\n\n    synthetic = dpgan.sample(10000)\n    synth_df = pd.DataFrame(synthetic, columns=X_data.columns)\n    synth_df[\"income\"] = np.random.choice([0, 1], size=len(synth_df))\n    synth_df = synth_df[columns]\n    X_train_dp, X_test_dp, y_train_dp, y_test_dp = preprocess_data(synth_df)\n    results[\"DP-GAN DNN\"] = train_dnn(X_train_dp, X_test_dp, y_train_dp, y_test_dp)\n\n    # 결과 출력\n    for model_name, metric in results.items():\n        print(f\"\\n{model_name}:\")\n        for m, v in metric.items():\n            print(f\"  {m}: {v:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:06:11.147795Z","iopub.execute_input":"2025-04-17T15:06:11.148092Z","iopub.status.idle":"2025-04-17T17:53:07.473021Z","shell.execute_reply.started":"2025-04-17T15:06:11.148066Z","shell.execute_reply":"2025-04-17T17:53:07.472266Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1744902371.736518    3289 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744902373.726278    3320 service.cc:148] XLA service 0x7cf14000aa40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744902373.726316    3320 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1744902373.880356    3320 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1744902374.298838    3320 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n\nOriginal DNN:\n  Accuracy: 0.8572\n  Precision: 0.7486\n  Recall: 0.6143\n  F1-score: 0.6748\n  AUC: 0.9098\n  Misclassification Error: 0.1428\n\nK-Anonymity DNN:\n  Accuracy: 0.8435\n  Precision: 0.7490\n  Recall: 0.4951\n  F1-score: 0.5961\n  AUC: 0.8925\n  Misclassification Error: 0.1565\n\nDP-GAN DNN:\n  Accuracy: 0.4865\n  Precision: 0.4924\n  Recall: 0.6445\n  F1-score: 0.5583\n  AUC: 0.4863\n  Misclassification Error: 0.5135\n","output_type":"stream"}],"execution_count":6}]}