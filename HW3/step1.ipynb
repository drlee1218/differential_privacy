{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":925175,"sourceType":"datasetVersion","datasetId":498911},{"sourceId":11727433,"sourceType":"datasetVersion","datasetId":7361556}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"rajnishe/facescrub-full\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Initialize Haar cascade\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n\n# Functions\ndef pixelate_face(image, x, y, w, h, blocks):\n    face = image[y:y+h, x:x+w]\n    (fh, fw) = face.shape[:2]\n    x_steps = np.linspace(0, fw, blocks + 1, dtype=\"int\")\n    y_steps = np.linspace(0, fh, blocks + 1, dtype=\"int\")\n    for i in range(1, len(y_steps)):\n        for j in range(1, len(x_steps)):\n            sx, sy = x_steps[j - 1], y_steps[i - 1]\n            ex, ey = x_steps[j], y_steps[i]\n            roi = face[sy:ey, sx:ex]\n            color = roi.mean(axis=(0, 1)).astype(\"uint8\")\n            cv2.rectangle(face, (sx, sy), (ex, ey), color.tolist(), -1)\n    image[y:y+h, x:x+w] = face\n    return image\n\ndef blur_face(image, x, y, w, h, kernel_size):\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n    face = image[y:y+h, x:x+w]\n    face_blur = cv2.GaussianBlur(face, (kernel_size, kernel_size), 0)\n    image[y:y+h, x:x+w] = face_blur\n    return image\n\n# Paths\nroot_dir = \"/kaggle/input/facescrub-full\"\noutput_blur = \"/kaggle/working/output_blur\"\noutput_pixel = \"/kaggle/working/output_pixel\"\nos.makedirs(output_blur, exist_ok=True)\nos.makedirs(output_pixel, exist_ok=True)\n\n# Parameters\nblur_kernels = [25, 45]\npixel_blocks = [25, 45]\n\n# Process\nfor subdir in ['actor_faces', 'actress_faces']:\n    input_path = os.path.join(root_dir, subdir)\n    for person in os.listdir(input_path):\n        person_path = os.path.join(input_path, person)\n        if not os.path.isdir(person_path):\n            continue\n        for img_file in os.listdir(person_path):\n            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                continue\n\n            image_path = os.path.join(person_path, img_file)\n            image = cv2.imread(image_path)\n            if image is None:\n                continue\n\n            # Create a small image for face detection (improve speed)\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            scale = 0.5\n            small_gray = cv2.resize(gray, (0, 0), fx=scale, fy=scale)\n            small_faces = face_cascade.detectMultiScale(small_gray, scaleFactor=1.1, minNeighbors=5)\n\n            # Restore to original size\n            faces = [(int(x/scale), int(y/scale), int(w/scale), int(h/scale)) for (x, y, w, h) in small_faces]\n            if len(faces) == 0:\n                continue\n\n            # Reuse pre-copied image\n            blur_imgs = {ksize: image.copy() for ksize in blur_kernels}\n            pixel_imgs = {block: image.copy() for block in pixel_blocks}\n\n            for (x, y, w, h) in faces:\n                for ksize in blur_kernels:\n                    blur_imgs[ksize] = blur_face(blur_imgs[ksize], x, y, w, h, ksize)\n                for block in pixel_blocks:\n                    pixel_imgs[block] = pixelate_face(pixel_imgs[block], x, y, w, h, block)\n\n            for ksize in blur_kernels:\n                blur_dir = os.path.join(output_blur, f\"ksize_{ksize}\", subdir, person)\n                os.makedirs(blur_dir, exist_ok=True)\n                cv2.imwrite(os.path.join(blur_dir, img_file), blur_imgs[ksize])\n\n            for block in pixel_blocks:\n                pixel_dir = os.path.join(output_pixel, f\"block_{block}\", subdir, person)\n                os.makedirs(pixel_dir, exist_ok=True)\n                cv2.imwrite(os.path.join(pixel_dir, img_file), pixel_imgs[block])\n\nprint(\"\\n Success!!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:34:56.198588Z","iopub.execute_input":"2025-05-18T12:34:56.199077Z","iopub.status.idle":"2025-05-18T13:20:14.621970Z","shell.execute_reply.started":"2025-05-18T12:34:56.199050Z","shell.execute_reply":"2025-05-18T13:20:14.621297Z"}},"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"\n Success!!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Ensure output directory exists for model saving\noutput_dir = \"/kaggle/working/models\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Image loading function (with special case handling)\ndef load_images_from_dirs(base_dirs, image_size=(64, 64), is_nested=False):\n    images = []\n    labels = []\n    label_map = {}\n    label_count = 0\n\n    for base_dir in base_dirs:\n        if is_nested:\n            # Handle nested structure: /base_dir/actor_faces/Aaron_Eckhart/...\n            for group_dir in os.listdir(base_dir):  # actor_faces, actress_faces\n                group_path = os.path.join(base_dir, group_dir)\n                if not os.path.isdir(group_path):\n                    continue\n\n                for label_name in os.listdir(group_path):  # e.g., Aaron_Eckhart\n                    person_path = os.path.join(group_path, label_name)\n                    if not os.path.isdir(person_path):\n                        continue\n\n                    if label_name not in label_map:\n                        label_map[label_name] = label_count\n                        label_count += 1\n                    label = label_map[label_name]\n\n                    for img_name in os.listdir(person_path):\n                        img_path = os.path.join(person_path, img_name)\n                        try:\n                            img = cv2.imread(img_path)\n                            if img is None:\n                                continue\n                            img = cv2.resize(img, image_size)\n                            images.append(img)\n                            labels.append(label)\n                        except Exception as e:\n                            print(f\"Error loading image {img_path}: {e}\")\n                            continue\n        else:\n            # Original structure: /base_dir/Aaron_Eckhart/...\n            for label_name in os.listdir(base_dir):\n                person_path = os.path.join(base_dir, label_name)\n                if not os.path.isdir(person_path):\n                    continue\n\n                if label_name not in label_map:\n                    label_map[label_name] = label_count\n                    label_count += 1\n                label = label_map[label_name]\n\n                for img_name in os.listdir(person_path):\n                    img_path = os.path.join(person_path, img_name)\n                    try:\n                        img = cv2.imread(img_path)\n                        if img is None:\n                            continue\n                        img = cv2.resize(img, image_size)\n                        images.append(img)\n                        labels.append(label)\n                    except Exception as e:\n                        print(f\"Error loading image {img_path}: {e}\")\n                        continue\n\n    return np.array(images), np.array(labels), label_map\n\n# CNN model definition\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2,2)),\n        Conv2D(64, (3,3), activation='relu'),\n        MaxPooling2D((2,2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Experiment settings\nexperiments = {\n    \"Original\": [\n        \"/kaggle/input/facescrub-full/actor_faces\",\n        \"/kaggle/input/facescrub-full/actress_faces\"\n    ],\n    \"Blur_K25\": [\"/kaggle/working/output_blur/ksize_25\"],\n    \"Blur_K45\": [\"/kaggle/working/output_blur/ksize_45\"],\n    \"Pixel_B25\": [\"/kaggle/working/output_pixel/block_25\"],\n    \"Pixel_B45\": [\"/kaggle/working/output_pixel/block_45\"]\n}\n\nresults = {}\n\n# Perform each experiment\nfor name, dirs in experiments.items():\n    print(f\"\\n--- Running Experiment: {name} ---\")\n\n    # Check if nested structure is needed\n    is_nested = name != \"Original\"\n    X, y, label_map = load_images_from_dirs(dirs, is_nested=is_nested)\n\n    if len(X) == 0:\n        print(f\"No images loaded for {name}, skipping...\")\n        continue\n\n    X = X / 255.0  # Normalize\n    y_cat = to_categorical(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)\n\n    model = create_cnn_model(X.shape[1:], y_cat.shape[1])\n    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_split=0.1)\n\n    # Save model for each experiment\n    model_save_path = os.path.join(output_dir, f\"{name.lower()}_face_model.h5\")\n    model.save(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test Accuracy for {name}: {test_acc:.4f}\")\n    results[name] = test_acc\n\n# Result summary\nprint(\"\\n=== Summary of Classification Accuracies ===\")\nfor name, acc in results.items():\n    print(f\"{name}: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T13:24:35.204588Z","iopub.execute_input":"2025-05-18T13:24:35.204899Z","iopub.status.idle":"2025-05-18T13:34:57.686432Z","shell.execute_reply.started":"2025-05-18T13:24:35.204875Z","shell.execute_reply":"2025-05-18T13:34:57.685721Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Experiment: Original ---\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0031 - loss: 6.2283 - val_accuracy: 0.0295 - val_loss: 5.4267\nEpoch 2/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0540 - loss: 5.0679 - val_accuracy: 0.1283 - val_loss: 4.4013\nEpoch 3/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1754 - loss: 3.9746 - val_accuracy: 0.2167 - val_loss: 3.8115\nEpoch 4/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2879 - loss: 3.2692 - val_accuracy: 0.2874 - val_loss: 3.4427\nEpoch 5/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3669 - loss: 2.8123 - val_accuracy: 0.3146 - val_loss: 3.2620\nEpoch 6/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4313 - loss: 2.4601 - val_accuracy: 0.3581 - val_loss: 3.0388\nEpoch 7/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4837 - loss: 2.2138 - val_accuracy: 0.3740 - val_loss: 2.9542\nEpoch 8/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5268 - loss: 1.9951 - val_accuracy: 0.3876 - val_loss: 2.8992\nEpoch 9/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5515 - loss: 1.8556 - val_accuracy: 0.3983 - val_loss: 2.9104\nEpoch 10/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5856 - loss: 1.7020 - val_accuracy: 0.4331 - val_loss: 2.8074\nModel saved to /kaggle/working/models/original_face_model.h5\nTest Accuracy for Original: 0.4244\n\n--- Running Experiment: Blur_K25 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.0046 - loss: 6.2013 - val_accuracy: 0.0214 - val_loss: 5.5814\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0362 - loss: 5.2971 - val_accuracy: 0.0843 - val_loss: 4.8027\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1011 - loss: 4.5285 - val_accuracy: 0.1278 - val_loss: 4.3132\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1797 - loss: 3.9312 - val_accuracy: 0.2031 - val_loss: 3.8838\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.2562 - loss: 3.4510 - val_accuracy: 0.2478 - val_loss: 3.6132\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3196 - loss: 3.0937 - val_accuracy: 0.2811 - val_loss: 3.4437\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3632 - loss: 2.8370 - val_accuracy: 0.3146 - val_loss: 3.3072\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4080 - loss: 2.6069 - val_accuracy: 0.3398 - val_loss: 3.2034\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4504 - loss: 2.4177 - val_accuracy: 0.3510 - val_loss: 3.1101\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4733 - loss: 2.2595 - val_accuracy: 0.3730 - val_loss: 3.0436\nModel saved to /kaggle/working/models/blur_k25_face_model.h5\nTest Accuracy for Blur_K25: 0.3648\n\n--- Running Experiment: Blur_K45 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.0033 - loss: 6.2264 - val_accuracy: 0.0150 - val_loss: 5.7650\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0255 - loss: 5.4876 - val_accuracy: 0.0473 - val_loss: 5.1073\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0767 - loss: 4.7319 - val_accuracy: 0.1118 - val_loss: 4.5173\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1479 - loss: 4.1365 - val_accuracy: 0.1715 - val_loss: 4.0932\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2158 - loss: 3.6903 - val_accuracy: 0.1987 - val_loss: 3.9367\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2777 - loss: 3.3624 - val_accuracy: 0.2328 - val_loss: 3.7297\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3221 - loss: 3.1009 - val_accuracy: 0.2702 - val_loss: 3.5907\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3534 - loss: 2.9062 - val_accuracy: 0.2903 - val_loss: 3.4612\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3955 - loss: 2.6817 - val_accuracy: 0.2954 - val_loss: 3.4487\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4205 - loss: 2.5560 - val_accuracy: 0.3216 - val_loss: 3.4098\nModel saved to /kaggle/working/models/blur_k45_face_model.h5\nTest Accuracy for Blur_K45: 0.3081\n\n--- Running Experiment: Pixel_B25 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.0047 - loss: 6.2008 - val_accuracy: 0.0233 - val_loss: 5.4618\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0502 - loss: 5.0838 - val_accuracy: 0.1022 - val_loss: 4.6349\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.1573 - loss: 4.1249 - val_accuracy: 0.2082 - val_loss: 3.8909\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2907 - loss: 3.2813 - val_accuracy: 0.2779 - val_loss: 3.5716\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3989 - loss: 2.7105 - val_accuracy: 0.3433 - val_loss: 3.2252\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4881 - loss: 2.2727 - val_accuracy: 0.3545 - val_loss: 3.1477\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5469 - loss: 1.9345 - val_accuracy: 0.4072 - val_loss: 2.9807\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5969 - loss: 1.6722 - val_accuracy: 0.4200 - val_loss: 2.9352\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6439 - loss: 1.4615 - val_accuracy: 0.4232 - val_loss: 2.9500\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6842 - loss: 1.2607 - val_accuracy: 0.4347 - val_loss: 2.9426\nModel saved to /kaggle/working/models/pixel_b25_face_model.h5\nTest Accuracy for Pixel_B25: 0.4260\n\n--- Running Experiment: Pixel_B45 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.0036 - loss: 6.2328 - val_accuracy: 0.0233 - val_loss: 5.5833\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0417 - loss: 5.2286 - val_accuracy: 0.0821 - val_loss: 4.6699\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.1330 - loss: 4.2436 - val_accuracy: 0.1814 - val_loss: 3.9925\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2317 - loss: 3.5798 - val_accuracy: 0.2402 - val_loss: 3.6750\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3031 - loss: 3.1636 - val_accuracy: 0.2734 - val_loss: 3.3919\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3574 - loss: 2.8548 - val_accuracy: 0.2989 - val_loss: 3.3158\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3966 - loss: 2.6044 - val_accuracy: 0.3405 - val_loss: 3.1404\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4447 - loss: 2.3648 - val_accuracy: 0.3548 - val_loss: 3.0488\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4810 - loss: 2.1949 - val_accuracy: 0.3660 - val_loss: 3.0337\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5132 - loss: 2.0480 - val_accuracy: 0.3817 - val_loss: 2.9690\nModel saved to /kaggle/working/models/pixel_b45_face_model.h5\nTest Accuracy for Pixel_B45: 0.3723\n\n=== Summary of Classification Accuracies ===\nOriginal: 0.4244\nBlur_K25: 0.3648\nBlur_K45: 0.3081\nPixel_B25: 0.4260\nPixel_B45: 0.3723\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom skimage.metrics import structural_similarity as ssim\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom concurrent.futures import ThreadPoolExecutor\nimport matplotlib.pyplot as plt\nimport gc\n\n# Ensure output directory exists for visualizations\noutput_dir = \"/kaggle/working\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Image loader with multithreading and sampling\ndef load_image(img_path, image_size):\n    img = cv2.imread(img_path)\n    if img is None:\n        print(f\"Failed to load image: {img_path}\")\n        return None, None\n    img = cv2.resize(img, image_size)\n    label = label_map[os.path.basename(os.path.dirname(img_path))]\n    return img, label\n\ndef load_original_images(base_dirs, image_size=(64, 64), max_samples=1000):\n    images, labels = [], []\n    paths = []\n    global label_map, label_count\n\n    for base_dir in base_dirs:\n        for label_name in os.listdir(base_dir):\n            person_path = os.path.join(base_dir, label_name)\n            if not os.path.isdir(person_path):\n                continue\n\n            if label_name not in label_map:\n                label_map[label_name] = label_count\n                label_count += 1\n            label = label_map[label_name]\n\n            img_names = os.listdir(person_path)\n            np.random.shuffle(img_names)\n            for img_name in img_names[:max_samples // len(base_dirs)]:\n                paths.append(os.path.join(person_path, img_name))\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(executor.map(lambda p: load_image(p, image_size), paths))\n    \n    for img, label in results:\n        if img is not None:\n            images.append(img)\n            labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\n# Traditional obfuscation: Pixelization\ndef apply_pixelization(images, block_size):\n    return np.array([cv2.blur(img, (block_size, block_size)) for img in images])\n\n# Traditional obfuscation: Gaussian blur\ndef apply_gaussian_blur(images, kernel_size):\n    return np.array([cv2.GaussianBlur(img, (kernel_size, kernel_size), 0) for img in images])\n\n# Differential Privacy Gaussian noise\ndef apply_dp_noise(images, epsilon, delta=1e-5, sensitivity=255.0):\n    sigma = np.sqrt(2 * np.log(1.25 / delta)) * sensitivity / epsilon\n    noisy_images = images + np.random.normal(loc=0.0, scale=sigma, size=images.shape)\n    return np.clip(noisy_images, 0, 255).astype(np.uint8)\n\n# Utility metrics (batch processing)\ndef compute_mse_ssim(originals, obfuscated, batch_size=100):\n    mse_list, ssim_list = [], []\n    for i in range(0, len(originals), batch_size):\n        batch_orig = originals[i:i + batch_size]\n        batch_obf = obfuscated[i:i + batch_size]\n        for orig, obf in zip(batch_orig, batch_obf):\n            mse_val = mean_squared_error(orig.flatten(), obf.flatten())\n            ssim_val = ssim(orig, obf, channel_axis=2, data_range=255)\n            mse_list.append(mse_val)\n            ssim_list.append(ssim_val)\n    return np.mean(mse_list), np.mean(ssim_list)\n\n# Visualize original vs obfuscated images\ndef visualize_images(original, obfuscated, method, param, num_samples=5):\n    indices = np.random.choice(len(original), num_samples, replace=False)\n    orig_samples = original[indices]\n    obf_samples = obfuscated[indices]\n    plt.figure(figsize=(10, 4))\n    for i in range(num_samples):\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(orig_samples[i])\n        plt.title(\"Original\")\n        plt.axis(\"off\")\n        plt.subplot(2, num_samples, i + 1 + num_samples)\n        plt.imshow(obf_samples[i])\n        plt.title(f\"{method} ({param})\")\n        plt.axis(\"off\")\n    plt.savefig(os.path.join(output_dir, f\"comparison_{method.lower().replace(' ', '_')}_{param}.png\"))\n    plt.close()\n\n# Load original images with sampling\nbase_dirs = [\"/kaggle/input/facescrub-full/actor_faces\", \"/kaggle/input/facescrub-full/actress_faces\"]\nlabel_map = {}\nlabel_count = 0\norig_images, orig_labels = load_original_images(base_dirs, max_samples=1000)\norig_images_norm = orig_images / 255.0\n\n# Encode labels\ny_cat = to_categorical(orig_labels)\n\n# Debug: Check shapes and number of classes\nprint(f\"y_cat shape: {y_cat.shape}\")\nprint(f\"Number of unique labels: {len(np.unique(orig_labels))}\")\n\n# Model paths for evaluation\nmodel_paths = {\n    \"Original\": \"/kaggle/working/models/original_face_model.h5\",\n    \"Blur_K25\": \"/kaggle/working/models/blur_k25_face_model.h5\",\n    \"Blur_K45\": \"/kaggle/working/models/blur_k45_face_model.h5\",\n    \"Pixel_B25\": \"/kaggle/working/models/pixel_b25_face_model.h5\",\n    \"Pixel_B45\": \"/kaggle/working/models/pixel_b45_face_model.h5\"\n}\n\n# Evaluation parameters\nepsilons = [0.1, 0.5, 1.0]\npixel_block_sizes = [25, 45]  # Match Step 2\ngaussian_kernels = [25, 45]   # Match Step 2\n\n# Evaluate all models\nresults = {}\n\nfor model_name, model_path in model_paths.items():\n    print(f\"\\n=== Evaluating with {model_name} Model ===\")\n    try:\n        model = load_model(model_path)\n    except Exception as e:\n        print(f\"Failed to load model {model_path}: {e}\")\n        continue\n\n    if model.output_shape[-1] != y_cat.shape[-1]:\n        print(f\"Warning: Model output classes ({model.output_shape[-1]}) do not match label classes ({y_cat.shape[-1]}) for {model_name}\")\n        continue\n\n    # Evaluate original dataset\n    print(\"\\n--- Original Dataset ---\")\n    test_loss, test_acc = model.evaluate(orig_images_norm, y_cat, verbose=0, batch_size=32)\n    print(f\"Classification Accuracy: {test_acc:.4f}\")\n    results[model_name] = {\"Original\": {\"accuracy\": test_acc * 100}}  # Store under model name\n\n    # Evaluate NP-Pix\n    print(\"\\n--- NP-Pix (Pixelization) ---\")\n    for block_size in pixel_block_sizes:\n        pixelized_images = apply_pixelization(orig_images, block_size)\n        pixelized_images_norm = pixelized_images / 255.0\n        visualize_images(orig_images, pixelized_images, \"NP-Pix\", f\"b={block_size}\")\n        test_loss, test_acc = model.evaluate(pixelized_images_norm, y_cat, verbose=0, batch_size=32)\n        mse_val, ssim_val = compute_mse_ssim(orig_images, pixelized_images, batch_size=32)\n        print(f\"\\n--- Block Size = {block_size} ---\")\n        print(f\"Classification Accuracy: {test_acc:.4f}\")\n        print(f\"MSE: {mse_val:.2f}\")\n        print(f\"SSIM: {ssim_val:.4f}\")\n        results[model_name][f\"NP_Pix_b{block_size}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n        del pixelized_images, pixelized_images_norm\n        gc.collect()\n\n    # Evaluate DP-Pix\n    print(\"\\n--- DP-Pix (Differential Privacy + Pixelization) ---\")\n    for epsilon in epsilons:\n        for block_size in pixel_block_sizes:\n            dp_images = apply_dp_noise(orig_images, epsilon)\n            dp_pixelized_images = apply_pixelization(dp_images, block_size)\n            dp_pixelized_images_norm = dp_pixelized_images / 255.0\n            visualize_images(orig_images, dp_pixelized_images, \"DP-Pix\", f\"b={block_size}_ε={epsilon}\")\n            test_loss, test_acc = model.evaluate(dp_pixelized_images_norm, y_cat, verbose=0, batch_size=32)\n            mse_val, ssim_val = compute_mse_ssim(orig_images, dp_pixelized_images, batch_size=32)\n            print(f\"\\n--- Block Size = {block_size}, ε = {epsilon} ---\")\n            print(f\"Classification Accuracy: {test_acc:.4f}\")\n            print(f\"MSE: {mse_val:.2f}\")\n            print(f\"SSIM: {ssim_val:.4f}\")\n            results[model_name][f\"DP_Pix_b{block_size}_ε{epsilon}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n            del dp_images, dp_pixelized_images, dp_pixelized_images_norm\n            gc.collect()\n\n    # Evaluate NP-Blur\n    print(\"\\n--- NP-Blur (Gaussian Blur) ---\")\n    for kernel_size in gaussian_kernels:\n        blurred_images = apply_gaussian_blur(orig_images, kernel_size)\n        blurred_images_norm = blurred_images / 255.0\n        visualize_images(orig_images, blurred_images, \"NP-Blur\", f\"k={kernel_size}\")\n        test_loss, test_acc = model.evaluate(blurred_images_norm, y_cat, verbose=0, batch_size=32)\n        mse_val, ssim_val = compute_mse_ssim(orig_images, blurred_images, batch_size=32)\n        print(f\"\\n--- Kernel Size = {kernel_size} ---\")\n        print(f\"Classification Accuracy: {test_acc:.4f}\")\n        print(f\"MSE: {mse_val:.2f}\")\n        print(f\"SSIM: {ssim_val:.4f}\")\n        results[model_name][f\"NP_Blur_k{kernel_size}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n        del blurred_images, blurred_images_norm\n        gc.collect()\n\n    # Evaluate DP-Blur\n    print(\"\\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\")\n    for epsilon in epsilons:\n        for kernel_size in gaussian_kernels:\n            dp_images = apply_dp_noise(orig_images, epsilon)\n            dp_blurred_images = apply_gaussian_blur(dp_images, kernel_size)\n            dp_blurred_images_norm = dp_blurred_images / 255.0\n            visualize_images(orig_images, dp_blurred_images, \"DP-Blur\", f\"k={kernel_size}_ε={epsilon}\")\n            test_loss, test_acc = model.evaluate(dp_blurred_images_norm, y_cat, verbose=0, batch_size=32)\n            mse_val, ssim_val = compute_mse_ssim(orig_images, dp_blurred_images, batch_size=32)\n            print(f\"\\n--- Kernel Size = {kernel_size}, ε = {epsilon} ---\")\n            print(f\"Classification Accuracy: {test_acc:.4f}\")\n            print(f\"MSE: {mse_val:.2f}\")\n            print(f\"SSIM: {ssim_val:.4f}\")\n            results[model_name][f\"DP_Blur_k{kernel_size}_ε{epsilon}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n            del dp_images, dp_blurred_images, dp_blurred_images_norm\n            gc.collect()\n\n# Result table\nprint(\"\\n=== Table 1: Accuracy (in %) of CNN Re-identification Attacks ===\")\nheader = [\"Dataset\", \"Original\"]\nheader.extend([f\"NP-Pix (b={b})\" for b in pixel_block_sizes])\nfor b in pixel_block_sizes:\n    header.extend([f\"DP-Pix (b={b}, ε={ε})\" for ε in epsilons])\nheader.extend([f\"NP-Blur (k={k})\" for k in gaussian_kernels])\nfor k in gaussian_kernels:\n    header.extend([f\"DP-Blur (k={k}, ε={ε})\" for ε in epsilons])\nprint(\"| \" + \" | \".join(header) + \" |\")\nprint(\"| \" + \" | \".join([\"-\" * len(h) for h in header]) + \" |\")\n\nfor model_name in model_paths.keys():\n    row = [model_name]\n    row.append(f\"{results[model_name]['Original']['accuracy']:.2f}\")  # Correct key\n    for b in pixel_block_sizes:\n        row.append(f\"{results[model_name][f'NP_Pix_b{b}']['accuracy']:.2f}\")\n    for b in pixel_block_sizes:\n        for ε in epsilons:\n            row.append(f\"{results[model_name][f'DP_Pix_b{b}_ε{ε}']['accuracy']:.2f}\")\n    for k in gaussian_kernels:\n        row.append(f\"{results[model_name][f'NP_Blur_k{k}']['accuracy']:.2f}\")\n    for k in gaussian_kernels:\n        for ε in epsilons:\n            row.append(f\"{results[model_name][f'DP_Blur_k{k}_ε{ε}']['accuracy']:.2f}\")\n    print(\"| \" + \" | \".join(row) + \" |\")\n\n# MSE and SSIM summary\nprint(\"\\n=== MSE and SSIM Results ===\")\nfor model_name in model_paths.keys():\n    for key, metrics in results[model_name].items():\n        if \"mse\" in metrics:\n            print(f\"{model_name}_{key}:\")\n            print(f\"  MSE: {metrics['mse']:.2f}\")\n            print(f\"  SSIM: {metrics['ssim']:.4f}\")\nprint(\"\\n success!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T13:35:12.007147Z","iopub.execute_input":"2025-05-18T13:35:12.007456Z","iopub.status.idle":"2025-05-18T16:07:38.033262Z","shell.execute_reply.started":"2025-05-18T13:35:12.007433Z","shell.execute_reply":"2025-05-18T16:07:38.032415Z"}},"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"Failed to load image: /kaggle/input/facescrub-full/actor_faces/Jean_Reno/Jean_Reno_54311_29031.gif\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"y_cat shape: (43147, 530)\nNumber of unique labels: 530\n\n=== Evaluating with Original Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.5736\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 25 ---\nClassification Accuracy: 0.0089\nMSE: 98.86\nSSIM: 0.2689\n\n--- Block Size = 45 ---\nClassification Accuracy: 0.0037\nMSE: 104.09\nSSIM: 0.2238\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- Block Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0017\nMSE: 105.59\nSSIM: 0.1739\n\n--- Block Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0018\nMSE: 105.49\nSSIM: 0.1785\n\n--- Block Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0019\nMSE: 106.43\nSSIM: 0.1784\n\n--- Block Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0019\nMSE: 106.30\nSSIM: 0.1805\n\n--- Block Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0025\nMSE: 107.32\nSSIM: 0.1838\n\n--- Block Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0020\nMSE: 107.20\nSSIM: 0.1829\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 25 ---\nClassification Accuracy: 0.0315\nMSE: 86.59\nSSIM: 0.4737\n\n--- Kernel Size = 45 ---\nClassification Accuracy: 0.0121\nMSE: 97.62\nSSIM: 0.3339\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- Kernel Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0019\nMSE: 105.62\nSSIM: 0.1645\n\n--- Kernel Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0017\nMSE: 105.60\nSSIM: 0.1778\n\n--- Kernel Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.30\nSSIM: 0.1763\n\n--- Kernel Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.49\nSSIM: 0.1843\n\n--- Kernel Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0027\nMSE: 107.02\nSSIM: 0.1911\n\n--- Kernel Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0022\nMSE: 107.39\nSSIM: 0.1922\n\n=== Evaluating with Blur_K25 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0038\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 25 ---\nClassification Accuracy: 0.0019\nMSE: 98.86\nSSIM: 0.2689\n\n--- Block Size = 45 ---\nClassification Accuracy: 0.0026\nMSE: 104.09\nSSIM: 0.2238\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- Block Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0024\nMSE: 105.58\nSSIM: 0.1739\n\n--- Block Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0024\nMSE: 105.50\nSSIM: 0.1785\n\n--- Block Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0024\nMSE: 106.45\nSSIM: 0.1784\n\n--- Block Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0023\nMSE: 106.31\nSSIM: 0.1805\n\n--- Block Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0021\nMSE: 107.32\nSSIM: 0.1838\n\n--- Block Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0026\nMSE: 107.19\nSSIM: 0.1828\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 25 ---\nClassification Accuracy: 0.0026\nMSE: 86.59\nSSIM: 0.4737\n\n--- Kernel Size = 45 ---\nClassification Accuracy: 0.0022\nMSE: 97.62\nSSIM: 0.3339\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- Kernel Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0020\nMSE: 105.61\nSSIM: 0.1645\n\n--- Kernel Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0024\nMSE: 105.61\nSSIM: 0.1778\n\n--- Kernel Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.29\nSSIM: 0.1764\n\n--- Kernel Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.49\nSSIM: 0.1843\n\n--- Kernel Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0019\nMSE: 107.03\nSSIM: 0.1911\n\n--- Kernel Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0020\nMSE: 107.41\nSSIM: 0.1922\n\n=== Evaluating with Blur_K45 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0034\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 25 ---\nClassification Accuracy: 0.0023\nMSE: 98.86\nSSIM: 0.2689\n\n--- Block Size = 45 ---\nClassification Accuracy: 0.0023\nMSE: 104.09\nSSIM: 0.2238\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- Block Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0025\nMSE: 105.59\nSSIM: 0.1740\n\n--- Block Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0026\nMSE: 105.49\nSSIM: 0.1785\n\n--- Block Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0025\nMSE: 106.45\nSSIM: 0.1784\n\n--- Block Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0026\nMSE: 106.31\nSSIM: 0.1805\n\n--- Block Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0026\nMSE: 107.31\nSSIM: 0.1838\n\n--- Block Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0025\nMSE: 107.19\nSSIM: 0.1828\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 25 ---\nClassification Accuracy: 0.0035\nMSE: 86.59\nSSIM: 0.4737\n\n--- Kernel Size = 45 ---\nClassification Accuracy: 0.0025\nMSE: 97.62\nSSIM: 0.3339\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- Kernel Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0024\nMSE: 105.61\nSSIM: 0.1645\n\n--- Kernel Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0025\nMSE: 105.60\nSSIM: 0.1778\n\n--- Kernel Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0025\nMSE: 106.29\nSSIM: 0.1764\n\n--- Kernel Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0027\nMSE: 106.49\nSSIM: 0.1843\n\n--- Kernel Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0026\nMSE: 107.03\nSSIM: 0.1912\n\n--- Kernel Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0025\nMSE: 107.39\nSSIM: 0.1922\n\n=== Evaluating with Pixel_B25 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0031\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 25 ---\nClassification Accuracy: 0.0026\nMSE: 98.86\nSSIM: 0.2689\n\n--- Block Size = 45 ---\nClassification Accuracy: 0.0018\nMSE: 104.09\nSSIM: 0.2238\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- Block Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0022\nMSE: 105.60\nSSIM: 0.1740\n\n--- Block Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0022\nMSE: 105.49\nSSIM: 0.1785\n\n--- Block Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.43\nSSIM: 0.1784\n\n--- Block Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.30\nSSIM: 0.1805\n\n--- Block Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0022\nMSE: 107.32\nSSIM: 0.1838\n\n--- Block Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0022\nMSE: 107.20\nSSIM: 0.1828\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 25 ---\nClassification Accuracy: 0.0037\nMSE: 86.59\nSSIM: 0.4737\n\n--- Kernel Size = 45 ---\nClassification Accuracy: 0.0028\nMSE: 97.62\nSSIM: 0.3339\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- Kernel Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0022\nMSE: 105.62\nSSIM: 0.1647\n\n--- Kernel Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0022\nMSE: 105.60\nSSIM: 0.1778\n\n--- Kernel Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0018\nMSE: 106.29\nSSIM: 0.1763\n\n--- Kernel Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.48\nSSIM: 0.1842\n\n--- Kernel Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0021\nMSE: 107.03\nSSIM: 0.1911\n\n--- Kernel Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0022\nMSE: 107.40\nSSIM: 0.1922\n\n=== Evaluating with Pixel_B45 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0034\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 25 ---\nClassification Accuracy: 0.0018\nMSE: 98.86\nSSIM: 0.2689\n\n--- Block Size = 45 ---\nClassification Accuracy: 0.0022\nMSE: 104.09\nSSIM: 0.2238\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- Block Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0021\nMSE: 105.60\nSSIM: 0.1740\n\n--- Block Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0021\nMSE: 105.50\nSSIM: 0.1785\n\n--- Block Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0017\nMSE: 106.45\nSSIM: 0.1784\n\n--- Block Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0020\nMSE: 106.30\nSSIM: 0.1805\n\n--- Block Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0019\nMSE: 107.32\nSSIM: 0.1838\n\n--- Block Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0019\nMSE: 107.19\nSSIM: 0.1829\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 25 ---\nClassification Accuracy: 0.0031\nMSE: 86.59\nSSIM: 0.4737\n\n--- Kernel Size = 45 ---\nClassification Accuracy: 0.0025\nMSE: 97.62\nSSIM: 0.3339\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- Kernel Size = 25, ε = 0.1 ---\nClassification Accuracy: 0.0023\nMSE: 105.62\nSSIM: 0.1645\n\n--- Kernel Size = 45, ε = 0.1 ---\nClassification Accuracy: 0.0021\nMSE: 105.61\nSSIM: 0.1778\n\n--- Kernel Size = 25, ε = 0.5 ---\nClassification Accuracy: 0.0017\nMSE: 106.29\nSSIM: 0.1763\n\n--- Kernel Size = 45, ε = 0.5 ---\nClassification Accuracy: 0.0023\nMSE: 106.49\nSSIM: 0.1843\n\n--- Kernel Size = 25, ε = 1.0 ---\nClassification Accuracy: 0.0025\nMSE: 107.03\nSSIM: 0.1911\n\n--- Kernel Size = 45, ε = 1.0 ---\nClassification Accuracy: 0.0023\nMSE: 107.41\nSSIM: 0.1922\n\n=== Table 1: Accuracy (in %) of CNN Re-identification Attacks ===\n| Dataset | Original | NP-Pix (b=25) | NP-Pix (b=45) | DP-Pix (b=25, ε=0.1) | DP-Pix (b=25, ε=0.5) | DP-Pix (b=25, ε=1.0) | DP-Pix (b=45, ε=0.1) | DP-Pix (b=45, ε=0.5) | DP-Pix (b=45, ε=1.0) | NP-Blur (k=25) | NP-Blur (k=45) | DP-Blur (k=25, ε=0.1) | DP-Blur (k=25, ε=0.5) | DP-Blur (k=25, ε=1.0) | DP-Blur (k=45, ε=0.1) | DP-Blur (k=45, ε=0.5) | DP-Blur (k=45, ε=1.0) |\n| ------- | -------- | ------------- | ------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------- | -------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- |\n| Original | 57.36 | 0.89 | 0.37 | 0.17 | 0.19 | 0.25 | 0.18 | 0.19 | 0.20 | 3.15 | 1.21 | 0.19 | 0.22 | 0.27 | 0.17 | 0.22 | 0.22 |\n| Blur_K25 | 0.38 | 0.19 | 0.26 | 0.24 | 0.24 | 0.21 | 0.24 | 0.23 | 0.26 | 0.26 | 0.22 | 0.20 | 0.22 | 0.19 | 0.24 | 0.22 | 0.20 |\n| Blur_K45 | 0.34 | 0.23 | 0.23 | 0.25 | 0.25 | 0.26 | 0.26 | 0.26 | 0.25 | 0.35 | 0.25 | 0.24 | 0.25 | 0.26 | 0.25 | 0.27 | 0.25 |\n| Pixel_B25 | 0.31 | 0.26 | 0.18 | 0.22 | 0.22 | 0.22 | 0.22 | 0.22 | 0.22 | 0.37 | 0.28 | 0.22 | 0.18 | 0.21 | 0.22 | 0.22 | 0.22 |\n| Pixel_B45 | 0.34 | 0.18 | 0.22 | 0.21 | 0.17 | 0.19 | 0.21 | 0.20 | 0.19 | 0.31 | 0.25 | 0.23 | 0.17 | 0.25 | 0.21 | 0.23 | 0.23 |\n\n=== MSE and SSIM Results ===\nOriginal_NP_Pix_b25:\n  MSE: 98.86\n  SSIM: 0.2689\nOriginal_NP_Pix_b45:\n  MSE: 104.09\n  SSIM: 0.2238\nOriginal_DP_Pix_b25_ε0.1:\n  MSE: 105.59\n  SSIM: 0.1739\nOriginal_DP_Pix_b45_ε0.1:\n  MSE: 105.49\n  SSIM: 0.1785\nOriginal_DP_Pix_b25_ε0.5:\n  MSE: 106.43\n  SSIM: 0.1784\nOriginal_DP_Pix_b45_ε0.5:\n  MSE: 106.30\n  SSIM: 0.1805\nOriginal_DP_Pix_b25_ε1.0:\n  MSE: 107.32\n  SSIM: 0.1838\nOriginal_DP_Pix_b45_ε1.0:\n  MSE: 107.20\n  SSIM: 0.1829\nOriginal_NP_Blur_k25:\n  MSE: 86.59\n  SSIM: 0.4737\nOriginal_NP_Blur_k45:\n  MSE: 97.62\n  SSIM: 0.3339\nOriginal_DP_Blur_k25_ε0.1:\n  MSE: 105.62\n  SSIM: 0.1645\nOriginal_DP_Blur_k45_ε0.1:\n  MSE: 105.60\n  SSIM: 0.1778\nOriginal_DP_Blur_k25_ε0.5:\n  MSE: 106.30\n  SSIM: 0.1763\nOriginal_DP_Blur_k45_ε0.5:\n  MSE: 106.49\n  SSIM: 0.1843\nOriginal_DP_Blur_k25_ε1.0:\n  MSE: 107.02\n  SSIM: 0.1911\nOriginal_DP_Blur_k45_ε1.0:\n  MSE: 107.39\n  SSIM: 0.1922\nBlur_K25_NP_Pix_b25:\n  MSE: 98.86\n  SSIM: 0.2689\nBlur_K25_NP_Pix_b45:\n  MSE: 104.09\n  SSIM: 0.2238\nBlur_K25_DP_Pix_b25_ε0.1:\n  MSE: 105.58\n  SSIM: 0.1739\nBlur_K25_DP_Pix_b45_ε0.1:\n  MSE: 105.50\n  SSIM: 0.1785\nBlur_K25_DP_Pix_b25_ε0.5:\n  MSE: 106.45\n  SSIM: 0.1784\nBlur_K25_DP_Pix_b45_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1805\nBlur_K25_DP_Pix_b25_ε1.0:\n  MSE: 107.32\n  SSIM: 0.1838\nBlur_K25_DP_Pix_b45_ε1.0:\n  MSE: 107.19\n  SSIM: 0.1828\nBlur_K25_NP_Blur_k25:\n  MSE: 86.59\n  SSIM: 0.4737\nBlur_K25_NP_Blur_k45:\n  MSE: 97.62\n  SSIM: 0.3339\nBlur_K25_DP_Blur_k25_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1645\nBlur_K25_DP_Blur_k45_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1778\nBlur_K25_DP_Blur_k25_ε0.5:\n  MSE: 106.29\n  SSIM: 0.1764\nBlur_K25_DP_Blur_k45_ε0.5:\n  MSE: 106.49\n  SSIM: 0.1843\nBlur_K25_DP_Blur_k25_ε1.0:\n  MSE: 107.03\n  SSIM: 0.1911\nBlur_K25_DP_Blur_k45_ε1.0:\n  MSE: 107.41\n  SSIM: 0.1922\nBlur_K45_NP_Pix_b25:\n  MSE: 98.86\n  SSIM: 0.2689\nBlur_K45_NP_Pix_b45:\n  MSE: 104.09\n  SSIM: 0.2238\nBlur_K45_DP_Pix_b25_ε0.1:\n  MSE: 105.59\n  SSIM: 0.1740\nBlur_K45_DP_Pix_b45_ε0.1:\n  MSE: 105.49\n  SSIM: 0.1785\nBlur_K45_DP_Pix_b25_ε0.5:\n  MSE: 106.45\n  SSIM: 0.1784\nBlur_K45_DP_Pix_b45_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1805\nBlur_K45_DP_Pix_b25_ε1.0:\n  MSE: 107.31\n  SSIM: 0.1838\nBlur_K45_DP_Pix_b45_ε1.0:\n  MSE: 107.19\n  SSIM: 0.1828\nBlur_K45_NP_Blur_k25:\n  MSE: 86.59\n  SSIM: 0.4737\nBlur_K45_NP_Blur_k45:\n  MSE: 97.62\n  SSIM: 0.3339\nBlur_K45_DP_Blur_k25_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1645\nBlur_K45_DP_Blur_k45_ε0.1:\n  MSE: 105.60\n  SSIM: 0.1778\nBlur_K45_DP_Blur_k25_ε0.5:\n  MSE: 106.29\n  SSIM: 0.1764\nBlur_K45_DP_Blur_k45_ε0.5:\n  MSE: 106.49\n  SSIM: 0.1843\nBlur_K45_DP_Blur_k25_ε1.0:\n  MSE: 107.03\n  SSIM: 0.1912\nBlur_K45_DP_Blur_k45_ε1.0:\n  MSE: 107.39\n  SSIM: 0.1922\nPixel_B25_NP_Pix_b25:\n  MSE: 98.86\n  SSIM: 0.2689\nPixel_B25_NP_Pix_b45:\n  MSE: 104.09\n  SSIM: 0.2238\nPixel_B25_DP_Pix_b25_ε0.1:\n  MSE: 105.60\n  SSIM: 0.1740\nPixel_B25_DP_Pix_b45_ε0.1:\n  MSE: 105.49\n  SSIM: 0.1785\nPixel_B25_DP_Pix_b25_ε0.5:\n  MSE: 106.43\n  SSIM: 0.1784\nPixel_B25_DP_Pix_b45_ε0.5:\n  MSE: 106.30\n  SSIM: 0.1805\nPixel_B25_DP_Pix_b25_ε1.0:\n  MSE: 107.32\n  SSIM: 0.1838\nPixel_B25_DP_Pix_b45_ε1.0:\n  MSE: 107.20\n  SSIM: 0.1828\nPixel_B25_NP_Blur_k25:\n  MSE: 86.59\n  SSIM: 0.4737\nPixel_B25_NP_Blur_k45:\n  MSE: 97.62\n  SSIM: 0.3339\nPixel_B25_DP_Blur_k25_ε0.1:\n  MSE: 105.62\n  SSIM: 0.1647\nPixel_B25_DP_Blur_k45_ε0.1:\n  MSE: 105.60\n  SSIM: 0.1778\nPixel_B25_DP_Blur_k25_ε0.5:\n  MSE: 106.29\n  SSIM: 0.1763\nPixel_B25_DP_Blur_k45_ε0.5:\n  MSE: 106.48\n  SSIM: 0.1842\nPixel_B25_DP_Blur_k25_ε1.0:\n  MSE: 107.03\n  SSIM: 0.1911\nPixel_B25_DP_Blur_k45_ε1.0:\n  MSE: 107.40\n  SSIM: 0.1922\nPixel_B45_NP_Pix_b25:\n  MSE: 98.86\n  SSIM: 0.2689\nPixel_B45_NP_Pix_b45:\n  MSE: 104.09\n  SSIM: 0.2238\nPixel_B45_DP_Pix_b25_ε0.1:\n  MSE: 105.60\n  SSIM: 0.1740\nPixel_B45_DP_Pix_b45_ε0.1:\n  MSE: 105.50\n  SSIM: 0.1785\nPixel_B45_DP_Pix_b25_ε0.5:\n  MSE: 106.45\n  SSIM: 0.1784\nPixel_B45_DP_Pix_b45_ε0.5:\n  MSE: 106.30\n  SSIM: 0.1805\nPixel_B45_DP_Pix_b25_ε1.0:\n  MSE: 107.32\n  SSIM: 0.1838\nPixel_B45_DP_Pix_b45_ε1.0:\n  MSE: 107.19\n  SSIM: 0.1829\nPixel_B45_NP_Blur_k25:\n  MSE: 86.59\n  SSIM: 0.4737\nPixel_B45_NP_Blur_k45:\n  MSE: 97.62\n  SSIM: 0.3339\nPixel_B45_DP_Blur_k25_ε0.1:\n  MSE: 105.62\n  SSIM: 0.1645\nPixel_B45_DP_Blur_k45_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1778\nPixel_B45_DP_Blur_k25_ε0.5:\n  MSE: 106.29\n  SSIM: 0.1763\nPixel_B45_DP_Blur_k45_ε0.5:\n  MSE: 106.49\n  SSIM: 0.1843\nPixel_B45_DP_Blur_k25_ε1.0:\n  MSE: 107.03\n  SSIM: 0.1911\nPixel_B45_DP_Blur_k45_ε1.0:\n  MSE: 107.41\n  SSIM: 0.1922\n\n success!\n","output_type":"stream"}],"execution_count":5}]}