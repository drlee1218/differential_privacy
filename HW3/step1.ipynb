{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":925175,"sourceType":"datasetVersion","datasetId":498911},{"sourceId":11727433,"sourceType":"datasetVersion","datasetId":7361556}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"rajnishe/facescrub-full\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Initialize Haar cascade\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n\n# Functions\ndef pixelate_face(image, x, y, w, h, blocks):\n    face = image[y:y+h, x:x+w]\n    (fh, fw) = face.shape[:2]\n    x_steps = np.linspace(0, fw, blocks + 1, dtype=\"int\")\n    y_steps = np.linspace(0, fh, blocks + 1, dtype=\"int\")\n    for i in range(1, len(y_steps)):\n        for j in range(1, len(x_steps)):\n            sx, sy = x_steps[j - 1], y_steps[i - 1]\n            ex, ey = x_steps[j], y_steps[i]\n            roi = face[sy:ey, sx:ex]\n            color = roi.mean(axis=(0, 1)).astype(\"uint8\")\n            cv2.rectangle(face, (sx, sy), (ex, ey), color.tolist(), -1)\n    image[y:y+h, x:x+w] = face\n    return image\n\ndef blur_face(image, x, y, w, h, kernel_size):\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n    face = image[y:y+h, x:x+w]\n    face_blur = cv2.GaussianBlur(face, (kernel_size, kernel_size), 0)\n    image[y:y+h, x:x+w] = face_blur\n    return image\n\n# Paths\nroot_dir = \"/kaggle/input/facescrub-full\"\noutput_blur = \"/kaggle/working/output_blur\"\noutput_pixel = \"/kaggle/working/output_pixel\"\nos.makedirs(output_blur, exist_ok=True)\nos.makedirs(output_pixel, exist_ok=True)\n\n# Parameters\nblur_kernels = [25, 45]\npixel_blocks = [25, 45]\n\n# Process\nfor subdir in ['actor_faces', 'actress_faces']:\n    input_path = os.path.join(root_dir, subdir)\n    for person in os.listdir(input_path):\n        person_path = os.path.join(input_path, person)\n        if not os.path.isdir(person_path):\n            continue\n        for img_file in os.listdir(person_path):\n            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                continue\n\n            image_path = os.path.join(person_path, img_file)\n            image = cv2.imread(image_path)\n            if image is None:\n                continue\n\n            # Create a small image for face detection (improve speed)\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            scale = 0.5\n            small_gray = cv2.resize(gray, (0, 0), fx=scale, fy=scale)\n            small_faces = face_cascade.detectMultiScale(small_gray, scaleFactor=1.1, minNeighbors=5)\n\n            # Restore to original size\n            faces = [(int(x/scale), int(y/scale), int(w/scale), int(h/scale)) for (x, y, w, h) in small_faces]\n            if len(faces) == 0:\n                continue\n\n            # Reuse pre-copied image\n            blur_imgs = {ksize: image.copy() for ksize in blur_kernels}\n            pixel_imgs = {block: image.copy() for block in pixel_blocks}\n\n            for (x, y, w, h) in faces:\n                for ksize in blur_kernels:\n                    blur_imgs[ksize] = blur_face(blur_imgs[ksize], x, y, w, h, ksize)\n                for block in pixel_blocks:\n                    pixel_imgs[block] = pixelate_face(pixel_imgs[block], x, y, w, h, block)\n\n            for ksize in blur_kernels:\n                blur_dir = os.path.join(output_blur, f\"ksize_{ksize}\", subdir, person)\n                os.makedirs(blur_dir, exist_ok=True)\n                cv2.imwrite(os.path.join(blur_dir, img_file), blur_imgs[ksize])\n\n            for block in pixel_blocks:\n                pixel_dir = os.path.join(output_pixel, f\"block_{block}\", subdir, person)\n                os.makedirs(pixel_dir, exist_ok=True)\n                cv2.imwrite(os.path.join(pixel_dir, img_file), pixel_imgs[block])\n\nprint(\"\\n Success!!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-16T12:01:56.500621Z","iopub.execute_input":"2025-05-16T12:01:56.500870Z","iopub.status.idle":"2025-05-16T12:53:00.467882Z","shell.execute_reply.started":"2025-05-16T12:01:56.500851Z","shell.execute_reply":"2025-05-16T12:53:00.467164Z"}},"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"\n Success!!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Ensure output directory exists for model saving\noutput_dir = \"/kaggle/working/models\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Image loading function (with special case handling)\ndef load_images_from_dirs(base_dirs, image_size=(64, 64), is_nested=False):\n    images = []\n    labels = []\n    label_map = {}\n    label_count = 0\n\n    for base_dir in base_dirs:\n        if is_nested:\n            # Handle nested structure: /base_dir/actor_faces/Aaron_Eckhart/...\n            for group_dir in os.listdir(base_dir):  # actor_faces, actress_faces\n                group_path = os.path.join(base_dir, group_dir)\n                if not os.path.isdir(group_path):\n                    continue\n\n                for label_name in os.listdir(group_path):  # e.g., Aaron_Eckhart\n                    person_path = os.path.join(group_path, label_name)\n                    if not os.path.isdir(person_path):\n                        continue\n\n                    if label_name not in label_map:\n                        label_map[label_name] = label_count\n                        label_count += 1\n                    label = label_map[label_name]\n\n                    for img_name in os.listdir(person_path):\n                        img_path = os.path.join(person_path, img_name)\n                        try:\n                            img = cv2.imread(img_path)\n                            if img is None:\n                                continue\n                            img = cv2.resize(img, image_size)\n                            images.append(img)\n                            labels.append(label)\n                        except Exception as e:\n                            print(f\"Error loading image {img_path}: {e}\")\n                            continue\n        else:\n            # Original structure: /base_dir/Aaron_Eckhart/...\n            for label_name in os.listdir(base_dir):\n                person_path = os.path.join(base_dir, label_name)\n                if not os.path.isdir(person_path):\n                    continue\n\n                if label_name not in label_map:\n                    label_map[label_name] = label_count\n                    label_count += 1\n                label = label_map[label_name]\n\n                for img_name in os.listdir(person_path):\n                    img_path = os.path.join(person_path, img_name)\n                    try:\n                        img = cv2.imread(img_path)\n                        if img is None:\n                            continue\n                        img = cv2.resize(img, image_size)\n                        images.append(img)\n                        labels.append(label)\n                    except Exception as e:\n                        print(f\"Error loading image {img_path}: {e}\")\n                        continue\n\n    return np.array(images), np.array(labels), label_map\n\n# CNN model definition\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2,2)),\n        Conv2D(64, (3,3), activation='relu'),\n        MaxPooling2D((2,2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Experiment settings\nexperiments = {\n    \"Original\": [\n        \"/kaggle/input/facescrub-full/actor_faces\",\n        \"/kaggle/input/facescrub-full/actress_faces\"\n    ],\n    \"Blur_K25\": [\"/kaggle/working/output_blur/ksize_25\"],\n    \"Blur_K45\": [\"/kaggle/working/output_blur/ksize_45\"],\n    \"Pixel_B25\": [\"/kaggle/working/output_pixel/block_25\"],\n    \"Pixel_B45\": [\"/kaggle/working/output_pixel/block_45\"]\n}\n\nresults = {}\n\n# Perform each experiment\nfor name, dirs in experiments.items():\n    print(f\"\\n--- Running Experiment: {name} ---\")\n\n    # Check if nested structure is needed\n    is_nested = name != \"Original\"\n    X, y, label_map = load_images_from_dirs(dirs, is_nested=is_nested)\n\n    if len(X) == 0:\n        print(f\"No images loaded for {name}, skipping...\")\n        continue\n\n    X = X / 255.0  # Normalize\n    y_cat = to_categorical(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)\n\n    model = create_cnn_model(X.shape[1:], y_cat.shape[1])\n    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_split=0.1)\n\n    # Save model for each experiment\n    model_save_path = os.path.join(output_dir, f\"{name.lower()}_face_model.h5\")\n    model.save(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test Accuracy for {name}: {test_acc:.4f}\")\n    results[name] = test_acc\n\n# Result summary\nprint(\"\\n=== Summary of Classification Accuracies ===\")\nfor name, acc in results.items():\n    print(f\"{name}: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T15:09:33.465105Z","iopub.execute_input":"2025-05-16T15:09:33.466119Z","iopub.status.idle":"2025-05-16T15:20:12.435074Z","shell.execute_reply.started":"2025-05-16T15:09:33.466086Z","shell.execute_reply":"2025-05-16T15:20:12.434060Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Experiment: Original ---\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.0083 - loss: 6.1223 - val_accuracy: 0.0605 - val_loss: 5.0157\nEpoch 2/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.1132 - loss: 4.5121 - val_accuracy: 0.2257 - val_loss: 3.8904\nEpoch 3/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3072 - loss: 3.2101 - val_accuracy: 0.3297 - val_loss: 3.2761\nEpoch 4/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.4541 - loss: 2.3952 - val_accuracy: 0.3954 - val_loss: 2.8951\nEpoch 5/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5639 - loss: 1.8496 - val_accuracy: 0.4374 - val_loss: 2.6951\nEpoch 6/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6528 - loss: 1.4601 - val_accuracy: 0.4577 - val_loss: 2.6885\nEpoch 7/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7062 - loss: 1.1930 - val_accuracy: 0.4705 - val_loss: 2.7767\nEpoch 8/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7605 - loss: 0.9512 - val_accuracy: 0.4762 - val_loss: 2.8252\nEpoch 9/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8018 - loss: 0.7705 - val_accuracy: 0.4939 - val_loss: 2.9750\nEpoch 10/10\n\u001b[1m971/971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8330 - loss: 0.6251 - val_accuracy: 0.4667 - val_loss: 3.2871\nModel saved to /kaggle/working/models/original_face_model.h5\nTest Accuracy for Original: 0.4835\n\n--- Running Experiment: Blur_K25 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.0041 - loss: 6.2076 - val_accuracy: 0.0249 - val_loss: 5.4829\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0595 - loss: 5.0525 - val_accuracy: 0.1249 - val_loss: 4.4273\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1788 - loss: 3.9731 - val_accuracy: 0.2185 - val_loss: 3.8669\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2965 - loss: 3.2581 - val_accuracy: 0.2734 - val_loss: 3.4911\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3810 - loss: 2.7880 - val_accuracy: 0.3213 - val_loss: 3.2489\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4478 - loss: 2.4156 - val_accuracy: 0.3708 - val_loss: 3.0301\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5080 - loss: 2.0950 - val_accuracy: 0.3903 - val_loss: 2.9628\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5635 - loss: 1.8605 - val_accuracy: 0.3817 - val_loss: 3.0795\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6098 - loss: 1.6488 - val_accuracy: 0.4053 - val_loss: 2.9782\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6402 - loss: 1.4850 - val_accuracy: 0.4293 - val_loss: 2.9702\nModel saved to /kaggle/working/models/blur_k25_face_model.h5\nTest Accuracy for Blur_K25: 0.4472\n\n--- Running Experiment: Blur_K45 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.0028 - loss: 6.2486 - val_accuracy: 0.0121 - val_loss: 5.9358\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0209 - loss: 5.6152 - val_accuracy: 0.0517 - val_loss: 5.0895\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0687 - loss: 4.8277 - val_accuracy: 0.0917 - val_loss: 4.6428\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1207 - loss: 4.3612 - val_accuracy: 0.1236 - val_loss: 4.3599\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1698 - loss: 4.0174 - val_accuracy: 0.1578 - val_loss: 4.1896\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2072 - loss: 3.7330 - val_accuracy: 0.1744 - val_loss: 4.0296\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2412 - loss: 3.5063 - val_accuracy: 0.2140 - val_loss: 3.8604\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2818 - loss: 3.3167 - val_accuracy: 0.2175 - val_loss: 3.8006\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3057 - loss: 3.1543 - val_accuracy: 0.2233 - val_loss: 3.7916\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3292 - loss: 2.9891 - val_accuracy: 0.2399 - val_loss: 3.7794\nModel saved to /kaggle/working/models/blur_k45_face_model.h5\nTest Accuracy for Blur_K45: 0.2524\n\n--- Running Experiment: Pixel_B25 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.0043 - loss: 6.2046 - val_accuracy: 0.0172 - val_loss: 5.5345\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0372 - loss: 5.2141 - val_accuracy: 0.0751 - val_loss: 4.7416\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1049 - loss: 4.4518 - val_accuracy: 0.1380 - val_loss: 4.2692\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1829 - loss: 3.8805 - val_accuracy: 0.2003 - val_loss: 3.8670\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.2589 - loss: 3.3987 - val_accuracy: 0.2242 - val_loss: 3.6833\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3190 - loss: 3.0869 - val_accuracy: 0.2801 - val_loss: 3.3908\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3672 - loss: 2.8098 - val_accuracy: 0.2689 - val_loss: 3.3959\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4114 - loss: 2.5696 - val_accuracy: 0.3136 - val_loss: 3.2757\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4442 - loss: 2.3837 - val_accuracy: 0.3408 - val_loss: 3.1579\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4765 - loss: 2.2057 - val_accuracy: 0.3609 - val_loss: 3.1047\nModel saved to /kaggle/working/models/pixel_b25_face_model.h5\nTest Accuracy for Pixel_B25: 0.3677\n\n--- Running Experiment: Pixel_B45 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.0039 - loss: 6.1966 - val_accuracy: 0.0256 - val_loss: 5.3653\nEpoch 2/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0590 - loss: 4.9874 - val_accuracy: 0.1223 - val_loss: 4.3724\nEpoch 3/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.1841 - loss: 3.9063 - val_accuracy: 0.2143 - val_loss: 3.7897\nEpoch 4/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2976 - loss: 3.2067 - val_accuracy: 0.2935 - val_loss: 3.4345\nEpoch 5/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3800 - loss: 2.7367 - val_accuracy: 0.3408 - val_loss: 3.1096\nEpoch 6/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4513 - loss: 2.3749 - val_accuracy: 0.3967 - val_loss: 2.8777\nEpoch 7/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5126 - loss: 2.0922 - val_accuracy: 0.4152 - val_loss: 2.7901\nEpoch 8/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5576 - loss: 1.8549 - val_accuracy: 0.4168 - val_loss: 2.8652\nEpoch 9/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5985 - loss: 1.6615 - val_accuracy: 0.4446 - val_loss: 2.7378\nEpoch 10/10\n\u001b[1m881/881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6269 - loss: 1.5062 - val_accuracy: 0.4507 - val_loss: 2.6699\nModel saved to /kaggle/working/models/pixel_b45_face_model.h5\nTest Accuracy for Pixel_B45: 0.4642\n\n=== Summary of Classification Accuracies ===\nOriginal: 0.4835\nBlur_K25: 0.4472\nBlur_K45: 0.2524\nPixel_B25: 0.3677\nPixel_B45: 0.4642\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom skimage.metrics import structural_similarity as ssim\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom concurrent.futures import ThreadPoolExecutor\nimport matplotlib.pyplot as plt\nimport gc\n\n# Ensure output directory exists for visualizations\noutput_dir = \"/kaggle/working\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Image loader with multithreading and sampling\ndef load_image(img_path, image_size):\n    img = cv2.imread(img_path)\n    if img is None:\n        print(f\"Failed to load image: {img_path}\")\n        return None, None\n    img = cv2.resize(img, image_size)\n    label = label_map[os.path.basename(os.path.dirname(img_path))]\n    return img, label\n\ndef load_original_images(base_dirs, image_size=(64, 64), max_samples=1000):\n    images, labels = [], []\n    paths = []\n    global label_map, label_count\n\n    for base_dir in base_dirs:\n        for label_name in os.listdir(base_dir):\n            person_path = os.path.join(base_dir, label_name)\n            if not os.path.isdir(person_path):\n                continue\n\n            if label_name not in label_map:\n                label_map[label_name] = label_count\n                label_count += 1\n            label = label_map[label_name]\n\n            img_names = os.listdir(person_path)\n            np.random.shuffle(img_names)\n            for img_name in img_names[:max_samples // len(base_dirs)]:\n                paths.append(os.path.join(person_path, img_name))\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(executor.map(lambda p: load_image(p, image_size), paths))\n    \n    for img, label in results:\n        if img is not None:\n            images.append(img)\n            labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\n# Traditional obfuscation: Pixelization\ndef apply_pixelization(images, block_size):\n    return np.array([cv2.blur(img, (block_size, block_size)) for img in images])\n\n# Traditional obfuscation: Gaussian blur\ndef apply_gaussian_blur(images, kernel_size):\n    return np.array([cv2.GaussianBlur(img, (kernel_size, kernel_size), 0) for img in images])\n\n# Differential Privacy Gaussian noise\ndef apply_dp_noise(images, epsilon, delta=1e-5, sensitivity=255.0):\n    sigma = np.sqrt(2 * np.log(1.25 / delta)) * sensitivity / epsilon\n    noisy_images = images + np.random.normal(loc=0.0, scale=sigma, size=images.shape)\n    return np.clip(noisy_images, 0, 255).astype(np.uint8)\n\n# Utility metrics (batch processing)\ndef compute_mse_ssim(originals, obfuscated, batch_size=100):\n    mse_list, ssim_list = [], []\n    for i in range(0, len(originals), batch_size):\n        batch_orig = originals[i:i + batch_size]\n        batch_obf = obfuscated[i:i + batch_size]\n        for orig, obf in zip(batch_orig, batch_obf):\n            mse_val = mean_squared_error(orig.flatten(), obf.flatten())\n            ssim_val = ssim(orig, obf, channel_axis=2, data_range=255)\n            mse_list.append(mse_val)\n            ssim_list.append(ssim_val)\n    return np.mean(mse_list), np.mean(ssim_list)\n\n# Visualize original vs obfuscated images\ndef visualize_images(original, obfuscated, method, param, num_samples=5):\n    indices = np.random.choice(len(original), num_samples, replace=False)\n    orig_samples = original[indices]\n    obf_samples = obfuscated[indices]\n    plt.figure(figsize=(10, 4))\n    for i in range(num_samples):\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(orig_samples[i])\n        plt.title(\"Original\")\n        plt.axis(\"off\")\n        plt.subplot(2, num_samples, i + 1 + num_samples)\n        plt.imshow(obf_samples[i])\n        plt.title(f\"{method} ({param})\")\n        plt.axis(\"off\")\n    plt.savefig(os.path.join(output_dir, f\"comparison_{method.lower().replace(' ', '_')}_{param}.png\"))\n    plt.close()\n\n# Load original images with sampling\nbase_dirs = [\"/kaggle/input/facescrub-full/actor_faces\", \"/kaggle/input/facescrub-full/actress_faces\"]\nlabel_map = {}\nlabel_count = 0\norig_images, orig_labels = load_original_images(base_dirs, max_samples=1000)\norig_images_norm = orig_images / 255.0\n\n# Encode labels\ny_cat = to_categorical(orig_labels)\n\n# Debug: Check shapes and number of classes\nprint(f\"y_cat shape: {y_cat.shape}\")\nprint(f\"Number of unique labels: {len(np.unique(orig_labels))}\")\n\n# Model paths for evaluation\nmodel_paths = {\n    \"Original\": \"/kaggle/working/models/original_face_model.h5\",\n    \"Blur_K25\": \"/kaggle/working/models/blur_k25_face_model.h5\",\n    \"Blur_K45\": \"/kaggle/working/models/blur_k45_face_model.h5\",\n    \"Pixel_B25\": \"/kaggle/working/models/pixel_b25_face_model.h5\",\n    \"Pixel_B45\": \"/kaggle/working/models/pixel_b45_face_model.h5\"\n}\n\n# Evaluation parameters (aligned with Step 2)\nepsilons = [0.1, 0.5, 1.0]\npixel_block_sizes = [25, 45]  # Match Pixel_B25, Pixel_B45\ngaussian_kernels = [25, 45]   # Match Blur_K25, Blur_K45\n\n# Evaluate all models\nresults = {}\n\nfor model_name, model_path in model_paths.items():\n    print(f\"\\n=== Evaluating with {model_name} Model ===\")\n    try:\n        model = load_model(model_path)\n    except Exception as e:\n        print(f\"Failed to load model {model_path}: {e}\")\n        continue\n\n    if model.output_shape[-1] != y_cat.shape[-1]:\n        print(f\"Warning: Model output classes ({model.output_shape[-1]}) do not match label classes ({y_cat.shape[-1]}) for {model_name}\")\n        continue\n\n    # Evaluate original dataset\n    print(\"\\n--- Original Dataset ---\")\n    test_loss, test_acc = model.evaluate(orig_images_norm, y_cat, verbose=0, batch_size=32)\n    print(f\"Classification Accuracy: {test_acc:.4f}\")\n    results[f\"{model_name}_Original\"] = {\"accuracy\": test_acc * 100}\n\n    # Evaluate NP-Pix\n    print(\"\\n--- NP-Pix (Pixelization) ---\")\n    for block_size in pixel_block_sizes:\n        pixelized_images = apply_pixelization(orig_images, block_size)\n        pixelized_images_norm = pixelized_images / 255.0\n        visualize_images(orig_images, pixelized_images, \"NP-Pix\", f\"b={block_size}\")\n        test_loss, test_acc = model.evaluate(pixelized_images_norm, y_cat, verbose=0, batch_size=32)\n        mse_val, ssim_val = compute_mse_ssim(orig_images, pixelized_images, batch_size=32)\n        print(f\"\\n--- Block Size = {block_size} ---\")\n        print(f\"Classification Accuracy: {test_acc:.4f}\")\n        print(f\"MSE: {mse_val:.2f}\")\n        print(f\"SSIM: {ssim_val:.4f}\")\n        results[f\"{model_name}_NP_Pix_b{block_size}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n        del pixelized_images, pixelized_images_norm\n        gc.collect()\n\n    # Evaluate DP-Pix\n    print(\"\\n--- DP-Pix (Differential Privacy + Pixelization) ---\")\n    for epsilon in epsilons:\n        for block_size in pixel_block_sizes:\n            dp_images = apply_dp_noise(orig_images, epsilon)\n            dp_pixelized_images = apply_pixelization(dp_images, block_size)\n            dp_pixelized_images_norm = dp_pixelized_images / 255.0\n            visualize_images(orig_images, dp_pixelized_images, \"DP-Pix\", f\"b={block_size}_ε={epsilon}\")\n            test_loss, test_acc = model.evaluate(dp_pixelized_images_norm, y_cat, verbose=0, batch_size=32)\n            mse_val, ssim_val = compute_mse_ssim(orig_images, dp_pixelized_images, batch_size=32)\n            print(f\"\\n--- Block Size = {block_size}, ε = {epsilon} ---\")\n            print(f\"Classification Accuracy: {test_acc:.4f}\")\n            print(f\"MSE: {mse_val:.2f}\")\n            print(f\"SSIM: {ssim_val:.4f}\")\n            results[f\"{model_name}_DP_Pix_b{block_size}_ε{epsilon}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n            del dp_images, dp_pixelized_images, dp_pixelized_images_norm\n            gc.collect()\n\n    # Evaluate NP-Blur\n    print(\"\\n--- NP-Blur (Gaussian Blur) ---\")\n    for kernel_size in gaussian_kernels:\n        blurred_images = apply_gaussian_blur(orig_images, kernel_size)\n        blurred_images_norm = blurred_images / 255.0\n        visualize_images(orig_images, blurred_images, \"NP-Blur\", f\"k={kernel_size}\")\n        test_loss, test_acc = model.evaluate(blurred_images_norm, y_cat, verbose=0, batch_size=32)\n        mse_val, ssim_val = compute_mse_ssim(orig_images, blurred_images, batch_size=32)\n        print(f\"\\n--- Kernel Size = {kernel_size} ---\")\n        print(f\"Classification Accuracy: {test_acc:.4f}\")\n        print(f\"MSE: {mse_val:.2f}\")\n        print(f\"SSIM: {ssim_val:.4f}\")\n        results[f\"{model_name}_NP_Blur_k{kernel_size}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n        del blurred_images, blurred_images_norm\n        gc.collect()\n\n    # Evaluate DP-Blur\n    print(\"\\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\")\n    for epsilon in epsilons:\n        for kernel_size in gaussian_kernels:\n            dp_images = apply_dp_noise(orig_images, epsilon)\n            dp_blurred_images = apply_gaussian_blur(dp_images, kernel_size)\n            dp_blurred_images_norm = dp_blurred_images / 255.0\n            visualize_images(orig_images, dp_blurred_images, \"DP-Blur\", f\"k={kernel_size}_ε={epsilon}\")\n            test_loss, test_acc = model.evaluate(dp_blurred_images_norm, y_cat, verbose=0, batch_size=32)\n            mse_val, ssim_val = compute_mse_ssim(orig_images, dp_blurred_images, batch_size=32)\n            print(f\"\\n--- Kernel Size = {kernel_size}, ε = {epsilon} ---\")\n            print(f\"Classification Accuracy: {test_acc:.4f}\")\n            print(f\"MSE: {mse_val:.2f}\")\n            print(f\"SSIM: {ssim_val:.4f}\")\n            results[f\"{model_name}_DP_Blur_k{kernel_size}_ε{epsilon}\"] = {\"accuracy\": test_acc * 100, \"mse\": mse_val, \"ssim\": ssim_val}\n            del dp_images, dp_blurred_images, dp_blurred_images_norm\n            gc.collect()\n\n# Result table\nprint(\"\\n=== Table 1: Accuracy (in %) of CNN Re-identification Attacks ===\")\nheader = [\"Dataset\", \"Original\"]\nheader.extend([f\"NP-Pix (b={b})\" for b in pixel_block_sizes])\nfor b in pixel_block_sizes:\n    header.extend([f\"DP-Pix (b={b}, ε={ε})\" for ε in epsilons])\nheader.extend([f\"NP-Blur (k={k})\" for k in gaussian_kernels])\nfor k in gaussian_kernels:\n    header.extend([f\"DP-Blur (k={k}, ε={ε})\" for ε in epsilons])\nprint(\"| \" + \" | \".join(header) + \" |\")\nprint(\"| \" + \" | \".join([\"-\" * len(h) for h in header]) + \" |\")\n\nfor model_name in model_paths.keys():\n    row = [model_name]\n    row.append(f\"{results[f'{model_name}_Original']['accuracy']:.2f}\")\n    for b in pixel_block_sizes:\n        row.append(f\"{results[f'{model_name}_NP_Pix_b{b}']['accuracy']:.2f}\")\n    for b in pixel_block_sizes:\n        for ε in epsilons:\n            row.append(f\"{results[f'{model_name}_DP_Pix_b{b}_ε{ε}']['accuracy']:.2f}\")\n    for k in gaussian_kernels:\n        row.append(f\"{results[f'{model_name}_NP_Blur_k{k}']['accuracy']:.2f}\")\n    for k in gaussian_kernels:\n        for ε in epsilons:\n            row.append(f\"{results[f'{model_name}_DP_Blur_k{k}_ε{ε}']['accuracy']:.2f}\")\n    print(\"| \" + \" | \".join(row) + \" |\")\n\n# MSE and SSIM summary\nprint(\"\\n=== MSE and SSIM Results ===\")\nfor key, metrics in results.items():\n    if \"mse\" in metrics:\n        print(f\"{key}:\")\n        print(f\"  MSE: {metrics['mse']:.2f}\")\n        print(f\"  SSIM: {metrics['ssim']:.4f}\")\nprint(\"\\n success!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:25:42.295670Z","iopub.execute_input":"2025-05-16T16:25:42.296085Z","iopub.status.idle":"2025-05-16T19:16:28.219316Z","shell.execute_reply.started":"2025-05-16T16:25:42.296056Z","shell.execute_reply":"2025-05-16T19:16:28.218233Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 16:25:45.489964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747412745.690192    4055 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747412745.748632    4055 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"Failed to load image: /kaggle/input/facescrub-full/actor_faces/Jean_Reno/Jean_Reno_54311_29031.gif\n","output_type":"stream"},{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: profile 'ICC Profile': 1000000h: invalid rendering intent\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"},{"name":"stdout","text":"y_cat shape: (43147, 530)\nNumber of unique labels: 530\n\n=== Evaluating with Original Model ===\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747412805.969253    4055 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1747412805.970064    4055 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\n--- Original Dataset ---\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1747412814.809668    4101 service.cc:148] XLA service 0x7aa92000e9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1747412814.810286    4101 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1747412814.810313    4101 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1747412814.932710    4101 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1747412816.432816    4101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Classification Accuracy: 0.7581\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 16 ---\nClassification Accuracy: 0.0224\nMSE: 91.05\nSSIM: 0.3656\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0019\nMSE: 105.61\nSSIM: 0.1621\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0022\nMSE: 106.31\nSSIM: 0.1701\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0022\nMSE: 107.07\nSSIM: 0.1805\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 99 ---\nClassification Accuracy: 0.0041\nMSE: 104.25\nSSIM: 0.2363\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0018\nMSE: 105.48\nSSIM: 0.1795\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0018\nMSE: 106.33\nSSIM: 0.1817\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0019\nMSE: 107.24\nSSIM: 0.1844\n\n=== Evaluating with Blur_K25 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0046\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 16 ---\nClassification Accuracy: 0.0029\nMSE: 91.05\nSSIM: 0.3656\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0017\nMSE: 105.62\nSSIM: 0.1620\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0021\nMSE: 106.31\nSSIM: 0.1701\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0016\nMSE: 107.07\nSSIM: 0.1804\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 99 ---\nClassification Accuracy: 0.0025\nMSE: 104.25\nSSIM: 0.2363\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0018\nMSE: 105.47\nSSIM: 0.1795\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0020\nMSE: 106.32\nSSIM: 0.1817\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0018\nMSE: 107.24\nSSIM: 0.1844\n\n=== Evaluating with Blur_K45 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0046\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 16 ---\nClassification Accuracy: 0.0040\nMSE: 91.05\nSSIM: 0.3656\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0020\nMSE: 105.61\nSSIM: 0.1618\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0020\nMSE: 106.32\nSSIM: 0.1702\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0025\nMSE: 107.07\nSSIM: 0.1804\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 99 ---\nClassification Accuracy: 0.0023\nMSE: 104.25\nSSIM: 0.2363\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0019\nMSE: 105.47\nSSIM: 0.1795\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0023\nMSE: 106.31\nSSIM: 0.1817\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0023\nMSE: 107.25\nSSIM: 0.1844\n\n=== Evaluating with Pixel_B25 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0054\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 16 ---\nClassification Accuracy: 0.0027\nMSE: 91.05\nSSIM: 0.3656\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0025\nMSE: 105.61\nSSIM: 0.1618\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0024\nMSE: 106.32\nSSIM: 0.1701\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0024\nMSE: 107.07\nSSIM: 0.1804\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 99 ---\nClassification Accuracy: 0.0019\nMSE: 104.25\nSSIM: 0.2363\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0024\nMSE: 105.49\nSSIM: 0.1795\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0024\nMSE: 106.31\nSSIM: 0.1817\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0024\nMSE: 107.25\nSSIM: 0.1844\n\n=== Evaluating with Pixel_B45 Model ===\n\n--- Original Dataset ---\nClassification Accuracy: 0.0054\n\n--- NP-Pix (Pixelization) ---\n\n--- Block Size = 16 ---\nClassification Accuracy: 0.0030\nMSE: 91.05\nSSIM: 0.3656\n\n--- DP-Pix (Differential Privacy + Pixelization) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0023\nMSE: 105.61\nSSIM: 0.1619\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0017\nMSE: 106.31\nSSIM: 0.1701\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0021\nMSE: 107.07\nSSIM: 0.1803\n\n--- NP-Blur (Gaussian Blur) ---\n\n--- Kernel Size = 99 ---\nClassification Accuracy: 0.0027\nMSE: 104.25\nSSIM: 0.2363\n\n--- DP-Blur (Differential Privacy + Gaussian Blur) ---\n\n--- ε = 0.1 ---\nClassification Accuracy: 0.0018\nMSE: 105.48\nSSIM: 0.1795\n\n--- ε = 0.5 ---\nClassification Accuracy: 0.0016\nMSE: 106.32\nSSIM: 0.1817\n\n--- ε = 1.0 ---\nClassification Accuracy: 0.0019\nMSE: 107.25\nSSIM: 0.1844\n\n=== Table 1: Accuracy (in %) of CNN Re-identification Attacks ===\n| Dataset | Original | NP-Pix (b=16) | DP-Pix (b=16, ε=0.1) | DP-Pix (b=16, ε=0.5) | DP-Pix (b=16, ε=1.0) | NP-Blur (k=99) | DP-Blur (k=99, ε=0.1) | DP-Blur (k=99, ε=0.5) | DP-Blur (k=99, ε=1.0) |\n| ------- | -------- | ------------- | -------------------- | -------------------- | -------------------- | -------------- | --------------------- | --------------------- | --------------------- |\n| Original | 75.81 | 2.24 | 0.19 | 0.22 | 0.22 | 0.41 | 0.18 | 0.18 | 0.19 |\n| Blur_K25 | 0.46 | 0.29 | 0.17 | 0.21 | 0.16 | 0.25 | 0.18 | 0.20 | 0.18 |\n| Blur_K45 | 0.46 | 0.40 | 0.20 | 0.20 | 0.25 | 0.23 | 0.19 | 0.23 | 0.23 |\n| Pixel_B25 | 0.54 | 0.27 | 0.25 | 0.24 | 0.24 | 0.19 | 0.24 | 0.24 | 0.24 |\n| Pixel_B45 | 0.54 | 0.30 | 0.23 | 0.17 | 0.21 | 0.27 | 0.18 | 0.16 | 0.19 |\n\n=== MSE and SSIM Results ===\nOriginal_NP_Pix_b16:\n  MSE: 91.05\n  SSIM: 0.3656\nOriginal_DP_Pix_b16_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1621\nOriginal_DP_Pix_b16_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1701\nOriginal_DP_Pix_b16_ε1.0:\n  MSE: 107.07\n  SSIM: 0.1805\nOriginal_NP_Blur_k99:\n  MSE: 104.25\n  SSIM: 0.2363\nOriginal_DP_Blur_k99_ε0.1:\n  MSE: 105.48\n  SSIM: 0.1795\nOriginal_DP_Blur_k99_ε0.5:\n  MSE: 106.33\n  SSIM: 0.1817\nOriginal_DP_Blur_k99_ε1.0:\n  MSE: 107.24\n  SSIM: 0.1844\nBlur_K25_NP_Pix_b16:\n  MSE: 91.05\n  SSIM: 0.3656\nBlur_K25_DP_Pix_b16_ε0.1:\n  MSE: 105.62\n  SSIM: 0.1620\nBlur_K25_DP_Pix_b16_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1701\nBlur_K25_DP_Pix_b16_ε1.0:\n  MSE: 107.07\n  SSIM: 0.1804\nBlur_K25_NP_Blur_k99:\n  MSE: 104.25\n  SSIM: 0.2363\nBlur_K25_DP_Blur_k99_ε0.1:\n  MSE: 105.47\n  SSIM: 0.1795\nBlur_K25_DP_Blur_k99_ε0.5:\n  MSE: 106.32\n  SSIM: 0.1817\nBlur_K25_DP_Blur_k99_ε1.0:\n  MSE: 107.24\n  SSIM: 0.1844\nBlur_K45_NP_Pix_b16:\n  MSE: 91.05\n  SSIM: 0.3656\nBlur_K45_DP_Pix_b16_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1618\nBlur_K45_DP_Pix_b16_ε0.5:\n  MSE: 106.32\n  SSIM: 0.1702\nBlur_K45_DP_Pix_b16_ε1.0:\n  MSE: 107.07\n  SSIM: 0.1804\nBlur_K45_NP_Blur_k99:\n  MSE: 104.25\n  SSIM: 0.2363\nBlur_K45_DP_Blur_k99_ε0.1:\n  MSE: 105.47\n  SSIM: 0.1795\nBlur_K45_DP_Blur_k99_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1817\nBlur_K45_DP_Blur_k99_ε1.0:\n  MSE: 107.25\n  SSIM: 0.1844\nPixel_B25_NP_Pix_b16:\n  MSE: 91.05\n  SSIM: 0.3656\nPixel_B25_DP_Pix_b16_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1618\nPixel_B25_DP_Pix_b16_ε0.5:\n  MSE: 106.32\n  SSIM: 0.1701\nPixel_B25_DP_Pix_b16_ε1.0:\n  MSE: 107.07\n  SSIM: 0.1804\nPixel_B25_NP_Blur_k99:\n  MSE: 104.25\n  SSIM: 0.2363\nPixel_B25_DP_Blur_k99_ε0.1:\n  MSE: 105.49\n  SSIM: 0.1795\nPixel_B25_DP_Blur_k99_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1817\nPixel_B25_DP_Blur_k99_ε1.0:\n  MSE: 107.25\n  SSIM: 0.1844\nPixel_B45_NP_Pix_b16:\n  MSE: 91.05\n  SSIM: 0.3656\nPixel_B45_DP_Pix_b16_ε0.1:\n  MSE: 105.61\n  SSIM: 0.1619\nPixel_B45_DP_Pix_b16_ε0.5:\n  MSE: 106.31\n  SSIM: 0.1701\nPixel_B45_DP_Pix_b16_ε1.0:\n  MSE: 107.07\n  SSIM: 0.1803\nPixel_B45_NP_Blur_k99:\n  MSE: 104.25\n  SSIM: 0.2363\nPixel_B45_DP_Blur_k99_ε0.1:\n  MSE: 105.48\n  SSIM: 0.1795\nPixel_B45_DP_Blur_k99_ε0.5:\n  MSE: 106.32\n  SSIM: 0.1817\nPixel_B45_DP_Blur_k99_ε1.0:\n  MSE: 107.25\n  SSIM: 0.1844\n\n Success!!\n","output_type":"stream"}],"execution_count":1}]}